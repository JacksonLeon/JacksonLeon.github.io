<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.82.0" />

  <title> [note] Andrew Ng Ml Note |  imlauzh</title>
  <meta name="description" content="A website built by Joseph Lau and host by Github pages.">
  <link rel="stylesheet" href="/blog/css/index.css">
  <link rel="stylesheet" href="/blog/css/classes.css">
  <link rel="canonical" href="/blog/posts/andrew-ng-ml-note/">
  <link rel="alternate" type="application/rss+xml" href="" title="imlauzh">
  
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">
  <link 
    rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" 
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" 
    crossorigin="anonymous">
  </link>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" 
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" 
    crossorigin="anonymous">
  </script>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" 
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" 
    crossorigin="anonymous" 
    onload="renderMathInElement(document.body);">
  </script>
</head>

<body>
  <header class="menus">
  

  <nav >
    
    <a href="/blog/"> Home</a>
    
    <a href="/blog/categories/"> Categories</a>
    
    <a href="/blog/tags/"> Tags</a>
    
    <a href="/blog/about/"> About</a>
    
    <a href="/blog/index.xml"> Subscribe</a>
    
  </nav>

  <nav class="fontawesome">
    
    <a href="https://github.com/imlauzh" target="_blank">
        <i title="GitHub" class="fab fa-github"></i>
    </a>
    
    
    <a href="/blog/index.xml" target="_blank">
        <i title="RSS" class="fas fa-rss"></i>
    </a>
    
  </nav>
  
  
  <div class="hidden description">A website built by Joseph Lau and host by Github pages.</div>
  
</header>

<article id="article">
  <header>
  
    <i class="fas fa-folder"></i>
    
    <a href="/blog/categories/note">Note</a>
    &nbsp;
    
  

    <h1 style="text-align: center;" >[note] Andrew Ng Ml Note</h1>
    <div class="post-meta">
    
      <time datetime="2019-04-05T17:03:30Z">April 05, 2019</time> &nbsp; 
    

    Lau &nbsp;

    
    
      <i class="far fa-eye"></i>
      <span id="/blog/posts/andrew-ng-ml-note/" class="leancloud_visitors" data-flag-title="[note] Andrew Ng Ml Note">
          <span class="leancloud-visitors-count">  </span>
      </span> &nbsp;
    
    

    
      <i class="far fa-clock"></i>
      
      
      

      
        45 min
      
      49 s
      &nbsp;
    
    </div>
  </header>

  <h2 id="什么是机器学习">什么是机器学习？</h2>
<ul>
<li>
<p>Arthur Samuel(1959)</p>
<p>编程使计算机自己和自己玩10^1000跳棋游戏</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc834f312e50.jpg" alt=""></p>
<ul>
<li>
<p>Tom Mitchell(1998)</p>
<p>计算机程序从经验E中学习任务T，并用度量P来衡量性能。条件是它由P定义的关于T的性能随着经验E而提高。</p>
<p>经验E=自己玩10^1000次跳棋</p>
<p>任务T=玩跳棋</p>
<p>性能度量P=与<strong>新</strong>对手玩跳棋时赢的概率</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc8323ea501d.jpg" alt="image-20190406171015837"></p>
<ul>
<li>邮件系统学习标记垃圾邮件</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc83246c6fb6.jpg" alt="image-20190406171657536"></p>
<ul>
<li>
<p>学习算法</p>
<ul>
<li>supervised learning
<ul>
<li>我们教计算机如何做一件事情</li>
<li>给标准答案</li>
</ul>
</li>
<li>unsupervised learning
<ul>
<li>计算机自己学习</li>
</ul>
</li>
<li>others
<ul>
<li>reinforcement learning</li>
<li>recommender systems</li>
</ul>
</li>
</ul>
</li>
<li>
<p>对应用学习算法的实用性建议（木匠）</p>
<p>如何去使用这些工具（机器学习算法）</p>
</li>
</ul>
<h2 id="监督学习">监督学习</h2>
<h3 id="example">Example</h3>
<ul>
<li>
<p>predict housing prices</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8324f7daff.jpg" alt="image-20190406201909053"></p>
<ul>
<li>直线拟合</li>
<li>二次函数或者二次多项式</li>
</ul>
<p>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做<strong>回归问题</strong>。我们试着推测出一个连续值的结果，即房子的价格。</p>
<p>一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。</p>
</li>
<li>
<p>breast cancer</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8325695ce7.jpg" alt="image-20190406202851738"></p>
<p>​	Malignant or benign？-&gt;(0,1)</p>
<p>​	这类机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。</p>
<p>​	分类指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。0 代表良性，这也是分类问题，属于多分类问题。可以用另外一种形式来表示（O表示malignant，X表示bengin）。</p>
<ul>
<li>
<p>多特征输入</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8326024fa1.jpg" alt="image-20190406203620654"></p>
<p>例如年龄、tumor大小、malignant or bengin……</p>
<p>良性区域or恶性区域</p>
<p>如何处理这些无穷多数量的特征？如何存储？你电脑的内存肯定不够用。**我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。**想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征，事实上，我们能用算法来处理它们。</p>
</li>
</ul>
</li>
</ul>
<h3 id="测验">测验</h3>
<p>现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题：</p>
<ol>
<li>你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你想预测接下来的三个月能卖多少件？</li>
<li>你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？</li>
</ol>
<p>那这两个问题，它们属于分类问题、还是回归问题?</p>
<p>问题一是一个回归问题，因为你知道，如果我有数千件货物，我会把它看成一个实数，一个连续的值。因此卖出的物品数，也是一个连续的值。</p>
<p>问题二是一个分类问题，因为我会把预测的值，<strong>用 0 来表示账户未被盗，用 1 表示账户曾经被盗过</strong>。所以我们根据账号是否被盗过，把它们定为0 或 1，然后用算法推测一个账号是 0 还是 1，因为只有少数的离散值，所以我把它归为分类问题。</p>
<p>以上就是监督学习的内容。</p>
<h3 id="总结">总结</h3>
<p>这节课我们介绍了监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。</p>
<h2 id="无监督学习">无监督学习</h2>
<p><img src="https://i.loli.net/2019/04/30/5cc83263b63bf.jpg" alt="image-20190406205004602"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc83268ac8aa.jpg" alt="image-20190406205023673"></p>
<p>在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？</p>
<p>针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。</p>
<ul>
<li>聚类应用</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc8326e8d92a.jpg" alt="image-20190406205247834"></p>
<p>谷歌新闻每天都在，收集非常多，非常多的网络的新闻内容。它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件，自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8327321c37.jpg" alt="image-20190406205624859"></p>
<ul>
<li>
<p>Cocktail party problem</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832792a760.jpg" alt="image-20190406210053078"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc8327cf19b3.jpg" alt="image-20190406210112864"></p>
</li>
</ul>
<h2 id="单变量线性回归">单变量线性回归</h2>
<p>Linear Regression with One Variable</p>
<h3 id="模型表示">模型表示</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc83283692bf.jpg" alt="image-20190406211745719"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc83288e5014.jpg" alt="image-20190406212146531"></p>
<p>一种可能的表达方式为：</p>
<p>$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$</p>
<p>因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
<h3 id="代价函数">代价函数</h3>
<p>我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8328faf434.jpg" alt="image-20190406214055225"></p>
<p>在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 $m = 47$。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：$h_\theta \left( x \right)=\theta_{0}+\theta_{1}x$。</p>
<p>接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的<strong>参数</strong>（<strong>parameters</strong>）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率和在$y$ 轴上的截距。</p>
<p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差</strong>（<strong>modeling error</strong>）。</p>
<p>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数</p>
<p>$$
J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}
$$</p>
<p>最小。(平方误差函数)</p>
<p>还有其他代价函数，不过平方误差函数对于线性回归是比较合理的。</p>
<p>我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/27ee0db04705fb20fab4574bb03064ab.png" alt="img"></p>
<h3 id="代价函数的直观理解一">代价函数的直观理解（一）</h3>
<p>在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8329450cff.jpg" alt=""></p>
<p>简化函数，先假设截距为0，计算简单的情况。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8329ad7de3.jpg" alt=""></p>
<p>当$\theta_1$改变时（$\theta_1=0,0.5,1,1.5,2,2.5&hellip;$），计算$J(\theta_1)$的函数图像。对于每一个$\theta_1$都有一个对应的$J(\theta_1)$，那么我们就可以画出来$J(\theta_1)$的函数图像，也就是代价函数的函数图像。为了找到一个最拟合数据的函数，需要改变$\theta_1$的值，从而使代价函数最小，也就是当$\theta_1=1$时可以拟合得到出((1,1),(2,2),(3,3))的函数。</p>
<h3 id="代价函数的直观理解二">代价函数的直观理解（二）</h3>
<p>前提：熟悉等高线图的绘制</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832a1a86c0.jpg" alt=""></p>
<p>代价函数的样子，<strong>等高线图可以更好的体现出函数图像上的点之间的关系</strong>，则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点（*碗底*）。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832a82a742.jpg" alt=""></p>
<p>通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。</p>
<p>当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。</p>
<p>我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\left( \theta_0,\theta_1,&hellip;&hellip;,\theta_n \right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_{0}, \theta_{1})$ 的最小值。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832b389486.jpg" alt=""></p>
<p>对$\theta_0$,$\theta_1$进行一些初步猜想，他们是多少并不重要，将他们全部设为0，我们不停的一点点改变$\theta_0$,$\theta_1$试图使$J(\theta_{0}, \theta_{1})$最小或者使局部最小值。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832c0a103a.jpg" alt=""></p>
<p><img src="https://i.loli.net/2019/04/30/5cc832ca74768.jpg" alt=""></p>
<p>想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p><strong>不同的起点，最终获得的局部最低点可能是不同的</strong>。</p>
<p>批量梯度下降（<strong>batch gradient descent</strong>）算法的公式为：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832ce153c2.jpg" alt=""></p>
<p>我们使用$:=$表示赋值，$=$表示判断为真的声明。</p>
<p>其中$\alpha$是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都<strong>同时</strong>让<strong>所有的参数减去学习速率乘以代价函数的导数</strong>，同时更新$\theta_0$,$\theta_1$。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832d48a96d.jpg" alt=""></p>
<p>在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新$\theta_0$和$\theta_1$ ，当 $j=0$ 和$j=1$时，会产生更新，所以你将更新$J\left( \theta_0 \right)$和$J\left( \theta_1 \right)$。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要<strong>同时更新$\theta_0$和$\theta_1$</strong>，我的意思是在这个等式中，我们要这样更新：</p>
<p>$\theta_0$:= $\theta_0$ ，并更新$\theta_1$:= $\theta_1$。</p>
<p>实现方法是：你应该计算公式右边的部分，通过那一部分计算出$\theta_0$和$\theta_1$的值，然后同时更新$\theta_0$和$\theta_1$。在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p>
<p>在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项：
$$
\alpha \frac{\partial }{\partial \theta_0}J(\theta_0,\theta_1)
$$</p>
<p>$$
\alpha \frac{\partial }{\partial \theta_1}J(\theta_0,\theta_1)
$$</p>
<p>如果你不熟悉微积分，不用担心，即使你之前没有看过微积分，或者没有接触过偏导数，在接下来的视频中，你会得到一切你需要知道，如何计算这个微分项的知识。</p>
<h3 id="梯度下降的直观理解">梯度下降的直观理解</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc832d8987ff.jpg" alt="image-20190411163204029"></p>
<p>梯度下降就是为了找到代价函数梯度最小的位置，也就是极小值，每次都要使$\theta$越来越接近0。</p>
<p>$\alpha$作为学习率，是每次梯度下降的步长，太小容易导致梯度下降太慢，太大容易导致梯度下降错过最小值，可能无法收敛，甚至分散。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832dd6ab33.jpg" alt="image-20190411164605244"></p>
<p>随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数的改变会变小一点点。所以<strong>我们即使用固定的学习率$\alpha$，我们也可以最终收敛到局部最小值。</strong></p>
<p>在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$\alpha$。</p>
<p>这就是梯度下降算法，你可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。</p>
<h3 id="梯度下降的线性回归">梯度下降的线性回归</h3>
<p>在以前的视频中我们谈到关于梯度下降算法，梯度下降是很常用的算法。在这段视频中，我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832e211dd3.jpg" alt="image-20190411170353256">
$$
\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial \theta_j} \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\<br>
=\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial \theta_j} \frac{1}{2m}\sum\limits_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2
$$
也就是对于$j$取$0,1$时：
$$
j=0:\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})
$$</p>
<p>$$
j=1:\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
$$</p>
<p>我们可以把梯度下降算法写成下面这个样子：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832e73a42e.jpg" alt="image-20190411172420834"></p>
<p>我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”<strong>批量梯度下降</strong>”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有$m$个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一&quot;批&quot;训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种&quot;批量&quot;型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。</p>
<p>如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数$J$最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数$J$的最小值，这是另一种称为正规方程(<strong>normal equations</strong>)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。</p>
<h3 id="whats-next">What’s next</h3>
<p>在接下来的一组视频中，我会对线性代数进行一个快速的复习回顾。</p>
<p>通过它们，你可以实现和使用更强大的线性回归模型。事实上，线性代数不仅仅在线性回归中应用广泛，它其中的矩阵和向量将有助于帮助我们实现之后更多的机器学习模型，并在计算上更有效率。正是因为这些矩阵和向量提供了一种有效的方式来组织大量的数据，特别是当我们处理巨大的训练集时，如果你不熟悉线性代数，如果你觉得线性代数看上去是一个复杂、可怕的概念，特别是对于之前从未接触过它的人，不必担心，事实上，为了实现机器学习算法，我们只需要一些非常非常基础的线性代数知识。通过接下来几个视频，你可以很快地学会所有你需要了解的线性代数知识。具体来说，为了帮助你判断是否有需要学习接下来的一组视频，我会讨论什么是矩阵和向量，谈谈如何加、减 、乘矩阵和向量，讨论逆矩阵和转置矩阵的概念。</p>
<h2 id="线性代数回顾linear-algebra-review">线性代数回顾(Linear Algebra Review)</h2>
<h3 id="矩阵和向量">矩阵和向量</h3>
<p>矩阵：二维数组、向量：一维数组</p>
<p>矩阵的维数即行数×列数</p>
<p>矩阵元素（矩阵项）：
$$
A=\left[\begin{matrix}1402 &amp; 191 \ 1371 &amp; 821 \ 949 &amp; 1437 \ 147 &amp; 1448\end{matrix}\right]
$$
$A_{ij}$指第$i$行，第$j$列的元素。</p>
<p>向量是一种特殊的矩阵，只有一列，讲义中的向量一般都是列向量，如： 
$$
y=\left[ \begin{matrix} 460 \ 232 \ 315 \ 178 \end{matrix} \right]
$$
为<strong>四维</strong>列向量（4×1）。</p>
<p>一般我们默认使用1索引：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8331283f50.jpg" alt="image-20190411225117460"></p>
<p>大写字母我们一般用来表示矩阵，例如矩阵$A$，小写字母用来表示数字。</p>
<h3 id="加法和标量乘法">加法和标量乘法</h3>
<p>矩阵的加法：行列数相等的可以加。</p>
<p>例如：
$$
\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]+\left[\begin{matrix}4&amp;0.5 \ 2&amp;5 \ 0&amp;1\end{matrix}\right]=\left[\begin{matrix}5&amp;0.5 \ 4&amp;10 \ 3&amp;2\end{matrix}\right]
$$
矩阵的乘法：每个元素都要乘</p>
<p>例如：
$$
3\times\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]=\left[\begin{matrix}3&amp;0 \ 6&amp;15 \ 9&amp;3\end{matrix}\right]=\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]\times3
$$</p>
<p>$$
\left[\begin{matrix}4&amp;0\6&amp;3\end{matrix}\right]/4=\frac{1}{4}\times\left[\begin{matrix}4&amp;0\6&amp;3\end{matrix}\right]=\left[\begin{matrix}1&amp;0\\frac{3}{2}&amp;\frac{3}{4}\end{matrix}\right]
$$</p>
<h3 id="矩阵向量乘法">矩阵向量乘法</h3>
<p>矩阵和向量的乘法如下：$m×n$的矩阵乘以$n×1$的向量，得到的是$m×1$的向量
$$
\left[\begin{matrix}1&amp;3\4&amp;0\2&amp;1\end{matrix}\right]\left[\begin{matrix}1\5\end{matrix}\right]=\left[\begin{matrix}16\4\7\end{matrix}\right]
$$</p>
<h3 id="矩阵乘法">矩阵乘法</h3>
<p>矩阵乘法：</p>
<p>$m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83318dd20a.jpg" alt="image-20190412182704551"></p>
<h3 id="矩阵乘法的性质">矩阵乘法的性质</h3>
<p>矩阵的乘法不满足交换律：$A×B≠B×A$</p>
<p>矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$</p>
<p>单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示，本讲义都用 $I$ 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0。如：</p>
<p>$AA^{-1}=A^{-1} A=I$</p>
<p>对于单位矩阵，有$A\cdot I=I\cdot A=A$</p>
<h3 id="矩阵逆转置">矩阵逆、转置</h3>
<p>矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$AA^{-1}=A^{-1}A=I$</p>
<p>矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$</p>
<p>定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记$A^T=B$。(有些书记为A'=B）</p>
<p>直观来看，将$A$的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到$A$的转置。</p>
<p>例：</p>
<p>$$
\left| \begin{matrix} a&amp; b \ c&amp; d \ e&amp; f \end{matrix} \right|^{T}=\left|\begin{matrix} a&amp; c &amp; e \ b&amp; d &amp; f \end{matrix} \right|
$$</p>
<p>矩阵的转置基本性质:</p>
<p>$$
\left( A\pm B \right)^{T}=A^T\pm B^T\ \left( A\times B \right)^{T}=B^{T}\times A^{T}\\left( A^T \right)^{T}=A \ \left( KA \right)^{T}=KA^{T}
$$
<strong>matlab</strong>中矩阵转置：直接打一撇，<code>x=y'</code>。</p>
<h2 id="多变量线性回归">多变量线性回归</h2>
<h3 id="多维特征">多维特征</h3>
<p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$( x_1,x_2,&hellip;,x_n )$。</p>
<p>增添更多特征后，我们引入一系列新的注释：</p>
<p>$n$ 代表特征的数量</p>
<p>$x^{( i )}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>比方说，上图的</p>
<p>${x}^{(2)}\text{=}\begin{bmatrix} 1416\ 3\ 2\ 40 \end{bmatrix}$，</p>
<p>${x}_{j}^{( i )}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。</p>
<p>如上图的$x_{2}^{( 2 )}=3,x_{3}^{( 2 )}=2$，</p>
<p>支持多变量的假设 $h$ 表示为：$h_{\theta}( x )=\theta_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$，</p>
<p>这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} ( x )=\theta_0x_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$</p>
<p>此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} ( x )=\theta^TX$，其中上标$T$代表矩阵转置。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8333c010e8.jpg" alt="image-20190416161948044"></p>
<h3 id="多变量梯度下降">多变量梯度下降</h3>
<p>如何利用梯度下降处理多元线性回归</p>
<p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：
$$
J(\theta_0,\theta_1,&hellip;,\theta_n)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
<img src="https://i.loli.net/2019/04/30/5cc833427cf84.jpg" alt="image-20190416170724732"></p>
<h3 id="梯度下降法实践1-特征缩放">梯度下降法实践1-特征缩放</h3>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8334a1afa2.jpg" alt="image-20190416193334095"></p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p>
<p>最简单的方法是令：$x_n=\frac{x_n-\mu_n}{S_n}$，其中 $\mu_n$是平均值，$S_n$是标准差。</p>
<p>使用特征缩放可以使梯度下降的速度变得很快，让梯度下降收敛所需的循环次数更少。</p>
<h3 id="梯度下降法实践2-学习率">梯度下降法实践2-学习率</h3>
<ul>
<li>如何确定梯度下降是否正确</li>
<li>如何选取学习率$\alpha$</li>
</ul>
<p>代价函数应该在每次循环后降低，表明梯度下降正确</p>
<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc833577a8da.jpg" alt="image-20190416194946523"></p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高；如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试这些学习率（2倍递增）：
$$
\alpha=0.001，0.03，0.01，0.1，0.3，1，3，10
$$</p>
<h3 id="特征和多项式回归">特征和多项式回归</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc8335dc9fe1.jpg" alt="image-20190417103501105"></p>
<p>$x_1=frontage$（临街宽度），$x_2=depth$（纵向深度），$x=frontage*depth=area$（面积），则： $h_{\theta}(x)=\theta_0+\theta_1x$。 线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_x^2$ 或者三次方模型： $h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3$。</p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：</p>
<p>$x_2=x_2^2,x_3=x_3^3$，从而将模型转化为线性回归模型。</p>
<p>根据函数图形特性，我们还可以使：</p>
<p>$h_{\theta}(x)=\theta_0+\theta_1(size)+\theta_2(size)^2$</p>
<p>或者:</p>
<p>$h_{\theta}(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{size}$</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83362cf28c.jpg" alt="image-20190417104451937"></p>
<p>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
<h3 id="正规方程">正规方程</h3>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。提供了一种求$\theta$的解析求法，所以与其使用迭代法求解，我们可以一次性求解$\theta$的最优值。</p>
<p>直观的理解：</p>
<p>对向量中的每一个变量求偏导，当等于0的时候求其变量值，这样就能得到一组向量。</p>
<p>正规方程不需要进行归一化，但是梯度下降时需要的。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83368d1fff.jpg" alt="image-20190417112743188">
$$
\theta=(X^TX)^{-1}X^Ty
$$
梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算$( X^TX )^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<h3 id="正规方程及不可逆性可选">正规方程及不可逆性（可选）</h3>
<p>正规方程 ( <strong>normal equation</strong> )，以及它们的不可逆性。</p>
<p>有些同学曾经问过我，当计算 $\theta$=<code>inv(X'X ) X'y</code> ，那对于矩阵$X&rsquo;X$的结果是不可逆的情况咋办呢? 如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为奇异或退化矩阵。</p>
<p>首先，看特征值里是否有一些多余的特征，像这些$x_1$和$x_2$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。</p>
<p><strong>$\theta =( X^TX )^{-1}X^Ty$ 的推导过程：</strong></p>
<p>$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$
其中：$h_{\theta}( x )=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$</p>
<p>将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}( X\theta -y)^2$ ，其中$X$为$m$行$n+1$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n+1$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换
$$
J(\theta )=\frac{1}{2}( X\theta -y)^T( X\theta -y )
\=\frac{1}{2}( \theta^TX^T-y^T)(X\theta -y )
\=\frac{1}{2}(\theta^TX^TX\theta -\theta^TX^Ty-y^TX\theta -y^Ty )
$$
接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:
$$
\frac{dAB}{dB}=A^T\<br>
\frac{dX^TAX}{dX}=2AX
$$
所以有:
$$
\frac{\partial J( \theta )}{\partial \theta }=\frac{1}{2}(2X^TX\theta -X^Ty -(y^TX )^T-0 )\<br>
=\frac{1}{2}(2X^TX\theta -X^Ty -X^Ty -0\<br>
=X^TX\theta -X^Ty
$$
令$\frac{\partial J( \theta )}{\partial \theta }=0$,</p>
<p>则有$\theta =( X^TX )^{-1}X^Ty$</p>
<h2 id="逻辑回归logistic-regression">逻辑回归(Logistic Regression)</h2>
<h3 id="分类问题">分类问题</h3>
<p>在分类问题中，你要预测的变量 $y$ 是离散的值，我们将学习一种叫做逻辑回归 (<strong>Logistic Regression</strong>) 的算法，这是目前最流行使用最广泛的一种学习算法。</p>
<p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc831f59ef6e.jpg" alt="image-20190419113034539"></p>
<p>这个算法的性质是：它的输出值永远在0到 1 之间。</p>
<p>顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 $y$ 取值离散的情况，如：1 0 0 1。</p>
<h3 id="假说表示">假说表示</h3>
<p>逻辑回归模型</p>
<p>$h_{\theta}=g(\theta^Tx)$ &amp; $g(z)=\frac{1}{1+e^{-z} }$ =&gt; $h_{\theta}=\frac{1}{1+e^{-\theta^Tx} }$</p>
<p>对模型的理解： $g(z)=\frac{1}{1+e^{-z} }$。</p>
<p>$h_\theta (x)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（**estimated probablity**）即$h_\theta(x)=P( y=1|x;\theta )$ 例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta (x)=0.7$，则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7=0.3。</p>
<h3 id="判定边界">判定边界</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc831fb61317.jpg" alt="image-20190419114449018"></p>
<p>e.g. $h_{\theta}=g(\theta_0+\theta_1x_1+\theta_2x_2)$,其中$\theta=\left[\begin{matrix} -3 &amp; 1 &amp; 1\end{matrix}\right]^T$</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832010ab1b.jpg" alt="image-20190419115043852"></p>


  
  <footer>
    <hr>
    
    <time datetime="2019-04-19T11:16:05Z">
      update @ April 19, 2019
    </time>
    <p></p>
    
    <div class="post-tags">
    
      <i class="fas fa-tags"></i>
      
        <a href="/blog/tags/ml">ML</a>
        &nbsp;
      
    
    </div>
  </footer>
  

  


  <div class="comments">



  <div class="comments-item" >
    
    
    
    <div id="vcomments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <script type="text/javascript">
      new Valine({
          el: '#vcomments',
          highlight: false,
          lang: "en",
          appId: "O9aoAtFO2Mk0VrPqbyHMHwah-gzGzoHsz",
          appKey: "1bF6m0SPiN3sk9TaGxPELdjY",
          placeholder: "Say Something......",
          requiredFields: ["nick","mail"],
          avatar: "robohash",
          visitor:  true ,
          recordIP: true
      });
    </script>
    <script>
      if(window.location.hash){
          var checkExist = setInterval(function() {
             if ($(window.location.hash).length) {
                $('html, body, article').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
                clearInterval(checkExist);
             }
          }, 100);
      }
    </script>
  </div>

</div>

</article>



  
  
  
</body>
<div class="foot">
  
  
    &copy; 2017 - 2021 &#183; 
    <a href="/">imlauzh</a> · Theme <a href="https://github.com/RainerChiang/simpleness">Simpleness</a> Powered by <a href="https://gohugo.io/">Hugo</a> &#183;
    <a href="#"><i class="fas fa-chevron-up"></i></a>
  

  
</div>

<script src="/blog/js/lazyload.min.js"></script>
<script>
  var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });
</script>


</html>
