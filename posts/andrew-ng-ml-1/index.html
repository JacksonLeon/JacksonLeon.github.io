<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.78.1" />

  <title> 吴恩达「机器学习」（二） |  imlauzh</title>
  <meta name="description" content="A website built by Joseph Lau and host by Github pages.">
  <link rel="stylesheet" href="/blog/css/index.css">
  <link rel="stylesheet" href="/blog/css/classes.css">
  <link rel="canonical" href="/blog/posts/andrew-ng-ml-1/">
  <link rel="alternate" type="application/rss+xml" href="" title="imlauzh">
  
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">
  <link 
    rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" 
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" 
    crossorigin="anonymous">
  </link>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" 
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" 
    crossorigin="anonymous">
  </script>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" 
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" 
    crossorigin="anonymous" 
    onload="renderMathInElement(document.body);">
  </script>
</head>

<body>
  <header class="menus">
  

  <nav >
    
    <a href="/blog/"> Home</a>
    
    <a href="/blog/categories/"> Categories</a>
    
    <a href="/blog/tags/"> Tags</a>
    
    <a href="/blog/about/"> About</a>
    
    <a href="/blog/index.xml"> Subscribe</a>
    
  </nav>

  <nav class="fontawesome">
    
    <a href="https://github.com/imlauzh" target="_blank">
        <i title="GitHub" class="fab fa-github"></i>
    </a>
    
    
    <a href="/blog/index.xml" target="_blank">
        <i title="RSS" class="fas fa-rss"></i>
    </a>
    
  </nav>
  
  
  <div class="hidden description">A website built by Joseph Lau and host by Github pages.</div>
  
</header>

<article id="article">
  <header>
  
    <i class="fas fa-folder"></i>
    
    <a href="/blog/categories/machine-learning">Machine Learning</a>
    &nbsp;
    
  

    <h1 style="text-align: center;" >吴恩达「机器学习」（二）</h1>
    <div class="post-meta">
    
      <time datetime="2019-04-12T18:46:34Z">April 12, 2019</time> &nbsp; 
    

     &nbsp;

    
    
      <i class="far fa-eye"></i>
      <span id="/blog/posts/andrew-ng-ml-1/" class="leancloud_visitors" data-flag-title="吴恩达「机器学习」（二）">
          <span class="leancloud-visitors-count">  </span>
      </span> &nbsp;
    
    

    
      <i class="far fa-clock"></i>
      
      
      

      
        12 min
      
      48 s
      &nbsp;
    
    </div>
  </header>

  <ul>
<li>多变量线性回归</li>
</ul>
<h2 id="多变量线性回归">多变量线性回归</h2>
<h3 id="多维特征">多维特征</h3>
<p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$( x_1,x_2,&hellip;,x_n )$。</p>
<p>增添更多特征后，我们引入一系列新的注释：</p>
<p>$n$ 代表特征的数量</p>
<p>$x^{( i )}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>比方说，上图的</p>
<p>${x}^{(2)}\text{=}\begin{bmatrix} 1416\ 3\ 2\ 40 \end{bmatrix}$，</p>
<p>${x}_{j}^{( i )}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。</p>
<p>如上图的$x_{2}^{( 2 )}=3,x_{3}^{( 2 )}=2$，</p>
<p>支持多变量的假设 $h$ 表示为：$h_{\theta}( x )=\theta_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$，</p>
<p>这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} ( x )=\theta_0x_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$</p>
<p>此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} ( x )=\theta^TX$，其中上标$T$代表矩阵转置。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8333c010e8.jpg" alt="image-20190416161948044"></p>
<h3 id="多变量梯度下降">多变量梯度下降</h3>
<p>如何利用梯度下降处理多元线性回归</p>
<p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：
$$
J(\theta_0,\theta_1,&hellip;,\theta_n)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
<img src="https://i.loli.net/2019/04/30/5cc833427cf84.jpg" alt="image-20190416170724732"></p>
<h3 id="梯度下降法实践1-特征缩放">梯度下降法实践1-特征缩放</h3>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8334a1afa2.jpg" alt="image-20190416193334095"></p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p>
<p>最简单的方法是令：$x_n=\frac{x_n-\mu_n}{S_n}$，其中 $\mu_n$是平均值，$S_n$是标准差。</p>
<p>使用特征缩放可以使梯度下降的速度变得很快，让梯度下降收敛所需的循环次数更少。</p>
<h3 id="梯度下降法实践2-学习率">梯度下降法实践2-学习率</h3>
<ul>
<li>如何确定梯度下降是否正确</li>
<li>如何选取学习率$\alpha$</li>
</ul>
<p>代价函数应该在每次循环后降低，表明梯度下降正确</p>
<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc833577a8da.jpg" alt="image-20190416194946523"></p>
<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高；如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试这些学习率（2倍递增）：
$$
\alpha=0.001，0.03，0.01，0.1，0.3，1，3，10
$$</p>
<h3 id="特征和多项式回归">特征和多项式回归</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc8335dc9fe1.jpg" alt="image-20190417103501105"></p>
<p>$x_1=frontage$（临街宽度），$x_2=depth$（纵向深度），$x=frontage*depth=area$（面积），则： $h_{\theta}(x)=\theta_0+\theta_1x$。 线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_x^2$ 或者三次方模型： $h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3$。</p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：</p>
<p>$x_2=x_2^2,x_3=x_3^3$，从而将模型转化为线性回归模型。</p>
<p>根据函数图形特性，我们还可以使：</p>
<p>$h_{\theta}(x)=\theta_0+\theta_1(size)+\theta_2(size)^2$</p>
<p>或者:</p>
<p>$h_{\theta}(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{size}$</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83362cf28c.jpg" alt="image-20190417104451937"></p>
<p>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
<h3 id="正规方程">正规方程</h3>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。提供了一种求$\theta$的解析求法，所以与其使用迭代法求解，我们可以一次性求解$\theta$的最优值。</p>
<p>直观的理解：</p>
<p>对向量中的每一个变量求偏导，当等于0的时候求其变量值，这样就能得到一组向量。</p>
<p>正规方程不需要进行归一化，但是梯度下降时需要的。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83368d1fff.jpg" alt="image-20190417112743188">
$$
\theta=(X^TX)^{-1}X^Ty
$$
梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算$( X^TX )^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<h3 id="正规方程及不可逆性可选">正规方程及不可逆性（可选）</h3>
<p>正规方程 ( <strong>normal equation</strong> )，以及它们的不可逆性。</p>
<p>有些同学曾经问过我，当计算 $\theta$=<code>inv(X'X ) X'y</code> ，那对于矩阵$X&rsquo;X$的结果是不可逆的情况咋办呢? 如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为奇异或退化矩阵。</p>
<p>首先，看特征值里是否有一些多余的特征，像这些$x_1$和$x_2$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。</p>
<p><strong>$\theta =( X^TX )^{-1}X^Ty$ 的推导过程：</strong></p>
<p>$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$
其中：$h_{\theta}( x )=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+&hellip;+\theta_nx_n$</p>
<p>将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}( X\theta -y)^2$ ，其中$X$为$m$行$n+1$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n+1$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换
$$
J(\theta )=\frac{1}{2}( X\theta -y)^T( X\theta -y )
\=\frac{1}{2}( \theta^TX^T-y^T)(X\theta -y )
\=\frac{1}{2}(\theta^TX^TX\theta -\theta^TX^Ty-y^TX\theta -y^Ty )
$$
接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:
$$
\frac{dAB}{dB}=A^T\<br>
\frac{dX^TAX}{dX}=2AX
$$
所以有:
$$
\frac{\partial J( \theta )}{\partial \theta }=\frac{1}{2}(2X^TX\theta -X^Ty -(y^TX )^T-0 )\<br>
=\frac{1}{2}(2X^TX\theta -X^Ty -X^Ty -0\<br>
=X^TX\theta -X^Ty
$$
令$\frac{\partial J( \theta )}{\partial \theta }=0$,</p>
<p>则有$\theta =( X^TX )^{-1}X^Ty$</p>

  
  <footer>
    <hr>
    
    <div class="post-tags">
    
      <i class="fas fa-tags"></i>
      
        <a href="/blog/tags/andrew-ng">Andrew NG</a>
        &nbsp;
      
    
    </div>
  </footer>
  

  

<div class="releated-post">
  <h3>Related Posts</h3>
  
  <i class="fas fa-paperclip"></i>
  <a href="/blog/posts/andrew-ng-ml-0/">吴恩达「机器学习」（一）</a>
  <br>
  
</div>


  <div class="comments">



  <div class="comments-item" >
    
    
    
    <div id="vcomments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <script type="text/javascript">
      new Valine({
          el: '#vcomments',
          highlight: false,
          lang: "en",
          appId: "O9aoAtFO2Mk0VrPqbyHMHwah-gzGzoHsz",
          appKey: "1bF6m0SPiN3sk9TaGxPELdjY",
          placeholder: "Say Something......",
          requiredFields: ["nick","mail"],
          avatar: "robohash",
          visitor:  true ,
          recordIP: true
      });
    </script>
    <script>
      if(window.location.hash){
          var checkExist = setInterval(function() {
             if ($(window.location.hash).length) {
                $('html, body, article').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
                clearInterval(checkExist);
             }
          }, 100);
      }
    </script>
  </div>

</div>

</article>



  
  
  
</body>
<div class="foot">
  
  
    &copy; 2017 - 2020 &#183; 
    <a href="/">imlauzh</a> · Theme <a href="https://github.com/RainerChiang/simpleness">Simpleness</a> Powered by <a href="https://gohugo.io/">Hugo</a> &#183;
    <a href="#"><i class="fas fa-chevron-up"></i></a>
  

  
</div>

<script src="/blog/js/lazyload.min.js"></script>
<script>
  var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });
</script>


</html>
