<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.79.0" />

  <title> Qiu Et Al__Pre Trained Models for Natural Language Processing |  imlauzh</title>
  <meta name="description" content="A website built by Joseph Lau and host by Github pages.">
  <link rel="stylesheet" href="/blog/css/index.css">
  <link rel="stylesheet" href="/blog/css/classes.css">
  <link rel="canonical" href="/blog/posts/qiu-et-al_2020_pre-trained-models-for-natural-language-processing/">
  <link rel="alternate" type="application/rss+xml" href="" title="imlauzh">
  
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">
  <link 
    rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" 
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" 
    crossorigin="anonymous">
  </link>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" 
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" 
    crossorigin="anonymous">
  </script>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" 
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" 
    crossorigin="anonymous" 
    onload="renderMathInElement(document.body);">
  </script>
</head>

<body>
  <header class="menus">
  

  <nav >
    
    <a href="/blog/"> Home</a>
    
    <a href="/blog/categories/"> Categories</a>
    
    <a href="/blog/tags/"> Tags</a>
    
    <a href="/blog/about/"> About</a>
    
    <a href="/blog/index.xml"> Subscribe</a>
    
  </nav>

  <nav class="fontawesome">
    
    <a href="https://github.com/imlauzh" target="_blank">
        <i title="GitHub" class="fab fa-github"></i>
    </a>
    
    
    <a href="/blog/index.xml" target="_blank">
        <i title="RSS" class="fas fa-rss"></i>
    </a>
    
  </nav>
  
  
  <div class="hidden description">A website built by Joseph Lau and host by Github pages.</div>
  
</header>

<article id="article">
  <header>
  

    <h1 style="text-align: center;" >Qiu Et Al__Pre Trained Models for Natural Language Processing</h1>
    <div class="post-meta">
    
      <time datetime="2020-12-12T15:57:21&#43;08:00">December 12, 2020</time> &nbsp; 
    

    Lau &nbsp;

    
    
      <i class="far fa-eye"></i>
      <span id="/blog/posts/qiu-et-al_2020_pre-trained-models-for-natural-language-processing/" class="leancloud_visitors" data-flag-title="Qiu Et Al__Pre Trained Models for Natural Language Processing">
          <span class="leancloud-visitors-count">  </span>
      </span> &nbsp;
    
    

    
      <i class="far fa-clock"></i>
      
      
      

      
        22 min
      
      12 s
      &nbsp;
    
    </div>
  </header>

  <h1 id="å¤§è‡´æ¡†æ¶">å¤§è‡´æ¡†æ¶</h1>
<ul>
<li>åŸºäºå››ä¸ªä¸åŒçš„æ–¹å‘ç»™PTMåˆ†ç±»</li>
<li>å¦‚ä½•å°†PTMçš„çŸ¥è¯†åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­å»</li>
<li>æœªæ¥çš„ç ”ç©¶ä¸­PTMæ½œåœ¨çš„å‡ ä¸ªæ–¹å‘</li>
<li>æœ¬æ–‡æ—¨åœ¨å¸®åŠ©ç ”ç©¶è€…ç†è§£ã€ä½¿ç”¨å’Œå‘å±•å„ç§NLPä»»åŠ¡ä¸‹çš„PTM</li>
</ul>
<h1 id="ä»‹ç»">ä»‹ç»</h1>
<p>ç°åœ¨NLPä»»åŠ¡ä¸»è¦æ˜¯åº”ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸€ä¸ªä¼˜åŠ¿æ˜¯é¿å…äº†[[Feature engineering|ç‰¹å¾å·¥ç¨‹]]é—®é¢˜ã€‚è€Œéç¥ç»ç½‘ç»œä¸€èˆ¬ç›¸å½“ä¾èµ–ç¦»æ•£çš„æ‰‹åŠ¨æ„å»ºçš„ç‰¹å¾ã€‚ç°å¦‚ä»Šç¥ç»ç½‘ç»œéƒ½æ˜¯ä½¿ç”¨ä½ç»´åº¦ã€å¯†é›†çš„å‘é‡ï¼ˆ[[Distributed Representation]]ï¼‰æ¥éšå«åœ°è¡¨ç¤ºè¡¨ç¤ºè¯­è¨€çš„å¥æ³•æˆ–è¯­æ³•ç‰¹å¾ã€‚</p>
<p>ç›¸æ¯”<a href="CV">[Computer Vision]</a>ï¼ŒNLPé¢†åŸŸçš„è¿›å±•å°±æ¯”è¾ƒå°äº†ï¼Œä¸»è¦æ˜¯å› ä¸ºå¤§å¤šæ•°ç›‘ç£NLPä»»åŠ¡çš„æ•°æ®é›†å¤ªå°ï¼ˆæœºå™¨ç¿»è¯‘é™¤å¤–ï¼‰,å¤§æ•°é‡çš„æ¨¡å‹å‚æ•°åœ¨å°æ•°æ®ä¸Šè®­ç»ƒç»å¸¸äº§ç”Ÿè¿‡æ‹Ÿåˆçš„ç°è±¡ï¼Œæ‰€ä»¥æ—©æœŸçš„NLPæ¨¡å‹å¤§å¤šæ˜¯çª„æ¨¡å‹ï¼Œä¸”åªæœ‰1-3å±‚ã€‚</p>
<p>é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤§è¯­æ–™åº“ä¸Šå¯ä»¥å­¦ä¹ åˆ°<strong>é€šç”¨çš„è¯­è¨€è¡¨ç¤º</strong>ï¼Œè€Œä¸”ä¸ç”¨ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ã€‚</p>
<p>ç¬¬ä¸€ä»£é¢„è®­ç»ƒæ¨¡å‹ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†æ›´å¥½åœ°å­¦ä¹ è¯åµŒå…¥è¡¨ç¤ºã€‚è®¡ç®—æ•ˆç‡ä¸Šæœ‰äº›ä½ï¼Œä¸è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸èƒ½æ•è·æ›´æ·±å±‚çš„ç‰¹å¾ä¿¡æ¯ï¼Œå­˜åœ¨ä¸€äº›é—®é¢˜ï¼špolysemous disambiguation, syntactic tructures, semantic roles, anaphoraã€‚
ä¸»è¦ä»£è¡¨ï¼šSkip-Gram [Distributed representations of words and phrases and their compositionality] and GloVe [GloVe: Global vectors for word representation].</p>
<p>ç¬¬äºŒä»£PTMsä¸»è¦é‡ç‚¹åœ¨äºå­¦ä¹ ä¸Šä¸‹æ–‡è¯åµŒå…¥
ä¾‹å¦‚ï¼šCoVe [Learned in translation: Contextualized word vectors], ELMo [Deep contextualized word representations],  [[Radford et al_Improving Language Understanding by Generative Pre-Training|OpenAI GPT]] and [[Devlin et al_2019_BERT|BERT]]ã€‚ä¸‹æ¸¸NLPä»»åŠ¡è¿˜æ˜¯éœ€è¦è¿™äº›è®­ç»ƒè¿‡çš„ç¼–ç å™¨æ¥è¡¨ç¤ºä¸Šä¸‹æ–‡çš„è¯ã€‚</p>
<h2 id="è¯¦ç»†æ¡†æ¶">è¯¦ç»†æ¡†æ¶</h2>
<ul>
<li>PTMsä¸­çš„èƒŒæ™¯çŸ¥è¯†ã€å¸¸ç”¨çš„ç¬¦å·</li>
<li>PTMsä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ä»¥åŠåˆ†ç±»</li>
<li>PTMsçš„æ‰©å±•</li>
<li>å¦‚ä½•åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡</li>
<li>PTMsçš„ç›¸å…³èµ„æº</li>
<li>NLPä»»åŠ¡çš„é›†åˆ</li>
<li>ç°åœ¨çš„æŒ‘æˆ˜ä»¥åŠæœªæ¥çš„æ–¹å‘</li>
</ul>
<h1 id="èƒŒæ™¯çŸ¥è¯†">èƒŒæ™¯çŸ¥è¯†</h1>
<h2 id="è¯­è¨€è¡¨ç¤ºå­¦ä¹ ">è¯­è¨€è¡¨ç¤ºå­¦ä¹ </h2>
<blockquote>
<p>a good representation should express general-purpose priors that are not task-specific but would be likely to be useful for a learning machine to solve AI-tasks.</p>
<p>ä¸€ä¸ªå¥½çš„è¡¨ç¤ºåº”è¯¥è¡¨è¾¾é€šç”¨çš„çŸ¥è¯†ï¼Œå¹¶ä¸æ˜¯é’ˆå¯¹ç‰¹å®šçš„ä»»åŠ¡ï¼Œä½†æ˜¯å¯¹æœºå™¨å­¦ä¹ å»è§£å†³AIä»»åŠ¡ä¼šå¾ˆæœ‰ç”¨ã€‚ &mdash;-Bengio</p>
</blockquote>
<p>å¯¹äºè¯­è¨€è¡¨ç¤ºæ¥è¯´ï¼Œ<strong>å¥½çš„è¡¨ç¤ºåº”è¯¥èƒ½å¤Ÿè·å–åˆ°éšå«çš„è¯­è¨€çš„è§„å¾‹ä»¥åŠå¸¸è¯†æ€§çŸ¥è¯†</strong>ï¼Œæ¯”å¦‚è¯´lexical meanings, syntactic structures, semantic roles, and even pragmatics(è¯­ç”¨å­¦)ã€‚åˆ†å¸ƒå¼è¡¨ç¤ºçš„<strong>æ ¸å¿ƒå°±æ˜¯åˆ©ç”¨ä½ç»´åº¦çš„å‘é‡æ¥æè¿°æ–‡æœ¬ä¸­çš„ä¸€éƒ¨åˆ†</strong>ï¼Œè¿™ä¸ªå‘é‡ä¸­çš„æ¯ä¸ªç»´åº¦éƒ½æ²¡æœ‰å¯¹åº”çš„æ„ä¹‰ï¼Œä½†æ˜¯æ•´ä¸ªå‘é‡å´è¡¨ç¤ºäº†ä¸€ä¸ªå…·ä½“çš„å«ä¹‰ï¼Œä¹Ÿè®¸æ¯ä¸ªå•ç‹¬çš„å‘é‡æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä¸è¿‡<strong>ç°åœ¨æˆ‘ä»¬æ— æ³•è§£é‡Šï¼Œå…¶å¯è§£é‡Šæ€§å¤ªä½</strong>ï¼Œå‡ ä¹æ²¡æœ‰ã€‚</p>
<p>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Generic%20Neural%20Architecture%20for%20NLP.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Generic%20Neural%20Architecture%20for%20NLP.pn)</a></p>
<p>ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸¤ç§è¯åµŒå…¥ï¼Œæœ‰æ²¡æœ‰ä¸Šä¸‹æ–‡è¯­æ„çš„ï¼Œä¹Ÿå°±æ˜¯å­—é¢æ„æ€ï¼Œ<strong>ä¼šä¸ä¼šæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯æ›´æ”¹å‘é‡çš„è¡¨ç¤º</strong>ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œ<strong>è‹¹æœ</strong>è¿™ä¸ªè¯è¯­ä»£è¡¨çš„è¯å‘é‡ä¼šä¸ä¼šæ¢äº†ä¸ªå¥å­ï¼Œå…¶è®­ç»ƒå‡ºæ¥çš„è¯åµŒå…¥å‘é‡å°±å‘ç”Ÿå˜åŒ–ã€‚</p>
<ul>
<li>
<p>Non-contextual Embeddings
è¿™ç±»è¯åµŒå…¥æœ‰ä¸¤ç§æ˜æ˜¾çš„å±€é™ï¼Œä¸€æ˜¯å› ä¸ºè¯åµŒå…¥æ˜¯é™æ€ä¸å˜çš„ï¼Œæ— æ³•è¡¨ç¤ºå¤šä¹‰æ€§çš„è¯è¯­ï¼›äºŒæ˜¯ï¼Œæ­¤è¡¨ä¸­<strong>æœªå‡ºç°çš„è¯è¯­æ— æ³•è¡¨ç¤ºï¼Œä¸€èˆ¬ä½¿ç”¨å­—ç¬¦çº§è¡¨ç¤ºæˆ–è€…å­è¯è¡¨ç¤º</strong>çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p>
</li>
<li>
<p>Contextual Embeddings
ä¸ºäº†è§£å†³ä¸Šé¢ä¸¤ä¸ªé—®é¢˜ï¼Œä¸»è¦é€šè¿‡åˆ†è¾¨è¯è¯­åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„è¯­æ„ï¼Œè¿™é‡Œæ˜¯å°†æ‰€æœ‰çš„è¯è¯­æˆ–è€…å­è¯æ”¾è¿›ä¸€ä¸ªç¥ç»ç¼–ç å™¨ä¸­å»è®¡ç®—æ¯ä¸ªè¯è¯­çš„ä¸Šä¸‹æ–‡è¯åµŒå…¥æˆ–è€…åŠ¨æ€è¯åµŒå…¥ã€‚</p>
</li>
</ul>
<h2 id="neural-contextual-encoders">Neural Contextual Encoders</h2>
<p>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Neural%20Contextual%20Encoders.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Neural%20Contextual%20Encoders.pn)</a></p>
<h3 id="åºåˆ—æ¨¡å‹">åºåˆ—æ¨¡å‹</h3>
<p><strong>åºåˆ—æ¨¡å‹é€šå¸¸æŒ‰é¡ºåºæ•è·å•è¯çš„å±€éƒ¨ä¸Šä¸‹æ–‡ã€‚</strong></p>
<ul>
<li>å·ç§¯æ¨¡å‹é€šè¿‡å·ç§¯æ“ä½œæŠŠé‚»å±…çš„æœ¬åœ°ä¿¡æ¯æ±‡æ€»ä»è€Œè·å¾—è¿™ä¸ªè¯è¯­çš„meaning</li>
<li>å¾ªç¯æ¨¡å‹é€šè¿‡short momeryæ¥è·å–ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ŒåŒå‘çš„LSTMæˆ–è€…æ˜¯GRUså¯ä»¥ä»ä¸¤ä¸ªæ–¹å‘æ”¶é›†ä¿¡æ¯ï¼Œä½†æ˜¯ä¼šå—åˆ°è¿œè·ç¦»ä¾èµ–é—®é¢˜çš„å½±å“ã€‚</li>
</ul>
<h3 id="éåºåˆ—å‹">éåºåˆ—å‹</h3>
<p>éåºåˆ—æ¨¡å‹é€šè¿‡<strong>é¢„å…ˆå®šä¹‰å¥½çš„æ ‘æˆ–è€…å›¾ç»“æ„ï¼ˆå¥æ³•æ ‘ã€è¯­æ³•å…³ç³»ï¼‰</strong> æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå…¶ä¸­æ„å»ºä¸€ä¸ªå¥½çš„å›¾ç»“æ„æ˜¯ä¸€ä¸ªæ¯”è¾ƒæœ‰æŒ‘æˆ˜çš„é—®é¢˜ï¼Œä¹Ÿæ¯”è¾ƒä¾èµ–å¤–éƒ¨çš„å·¥å…·æˆ–è€…æ˜¯çŸ¥è¯†ï¼Œä¾‹å¦‚ä¾å­˜è§£æã€‚</p>
<p>å®é™…ä¸Šï¼Œæ›´åŠ ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨å…¨è¿æ¥å›¾ï¼Œè¿æ¥çš„æƒé‡æ˜¯<strong>é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒåŠ¨æ€è®¡ç®—çš„</strong>ï¼Œèƒ½å¤Ÿéšå«åœ°æ˜¾ç¤ºè¯ä¸è¯ä¹‹é—´çš„å…³ç³»ï¼Œä¸€ä¸ªæ¯”è¾ƒæˆåŠŸçš„ä¾‹å­æ˜¯[[Vaswani et al_2017_Attention Is All You Need|Transformer]]ï¼Œä¸è¿‡ä¹Ÿä½¿ç”¨äº†ä¸€äº›å…¶ä»–çš„æ¨¡å—ï¼š[[Positional Embeddings]]ã€[[Feed-Forward Network(FFN)]]ã€[[Layer Normalization]]ã€[[Residual Connections]]ç­‰ç­‰ã€‚</p>
<p>åºåˆ—æ¨¡å‹éš¾ä»¥è·å–é•¿å°¾çš„è¯ä¿¡æ¯ï¼Œä½†æ˜¯æ˜“äºè®­ç»ƒï¼Œå¤šç§NLPä»»åŠ¡ä¹Ÿå¯ä»¥æ‹¿åˆ°ä¸é”™çš„æˆç»©ã€‚</p>
<p><strong>Transformerå¯ä»¥ç›´æ¥å¯¹åºåˆ—ä¸­æ¯ä¸¤ä¸ªå•è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ›´å¼ºå¤§å¹¶ä¸”æ›´é€‚åˆå¯¹è¯­è¨€çš„è¿œç¨‹ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚</strong></p>
<blockquote>
<p>However, due to its heavy structure and less model bias, the Transformer usually requires a large training corpus and is easy to overfit on small or modestly-sized datasets.</p>
<p>Reference: [[Radford et al_Improving Language Understanding by Generative Pre-Training|GPT]]</p>
</blockquote>
<p>ç”±äºtransformerå…¶å¼ºå¤§çš„èƒ½åŠ›ï¼Œä¹Ÿæ˜¯ç›®å‰ä¸»æµçš„PTMså¸¸ç”¨çš„ç»“æ„ã€‚</p>
<h2 id="ä¸ºä»€ä¹ˆéœ€è¦é¢„è®­ç»ƒ">ä¸ºä»€ä¹ˆéœ€è¦é¢„è®­ç»ƒï¼Ÿ</h2>
<ul>
<li>ç”±äºå¤§éƒ¨åˆ†NLPä»»åŠ¡éœ€è¦å……åˆ†è®­ç»ƒæ¨¡å‹ä»è€Œé˜²æ­¢è¿‡æ‹Ÿåˆ</li>
<li>è€Œè¿™å°±éœ€è¦å¤§è§„æ¨¡çš„æ ‡æ³¨æ•°æ®é›†</li>
<li>ä½†æ˜¯å¯¹äºå¥æ³•å’Œè¯­ä¹‰ç­‰ä»»åŠ¡æ¥è¯´ï¼Œäººå·¥æ ‡æ³¨ç›¸å½“æ˜‚è´µ</li>
<li>æ‰€ä»¥æƒ³è¦å…ˆç”¨å¤§è§„æ¨¡çš„æ— æ ‡æ³¨æ•°æ®å­¦ä¹ ä¸€ä¸ªå¥½çš„è¡¨ç¤ºï¼Œå†ç”¨äºä¸‹æ¸¸ä»»åŠ¡ä¸­</li>
<li>ä¼˜ç‚¹ï¼š
<ul>
<li>å¯ä»¥å­¦ä¹ åˆ°é€šç”¨çš„è¯­è¨€è¡¨ç¤º</li>
<li>æä¾›äº†æ›´å¥½åœ°æ¨¡å‹åˆå§‹åŒ–</li>
<li>é¿å…äº†åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ</li>
</ul>
</li>
</ul>
<h2 id="nlpé¢„è®­ç»ƒå†å²">NLPé¢„è®­ç»ƒå†å²</h2>
<ul>
<li>ç¬¬ä¸€ä»£ï¼šé¢„è®­ç»ƒè¯åµŒå…¥
<ul>
<li>æŠŠè¯è¯­è¡¨ç¤ºæˆç¨ å¯†å‘é‡ï¼Œone-hotæ˜¯ç¨€ç–çš„</li>
<li>pair-wise ranking taskè€Œä¸æ˜¯è¯­è¨€æ¨¡å‹</li>
<li>Mikolovæå‡ºå¹¶ä¸éœ€è¦æ·±åº¦ç¥ç»ç½‘ç»œæ¥æ„å»ºå¥½çš„è¯åµŒå…¥ï¼Œå¹¶æå‡ºCBOWå’Œskip-gramæ¨¡å‹</li>
<li>Word2Vecã€GloVe</li>
<li>éƒ½æ˜¯ä¸Šä¸‹æ–‡æ— å…³çš„ï¼Œå¤§å¤šæ•°éƒ½æ˜¯ä½¿ç”¨æµ…å±‚æ¨¡å‹è®­ç»ƒï¼Œæœªè®­ç»ƒåˆ°çš„è¿˜éœ€è¦æ¨¡å‹ä»å¤´è®­ç»ƒ</li>
</ul>
</li>
<li>ç¬¬äºŒä»£ï¼šä¸Šä¸‹æ–‡ç¼–ç å™¨
<ul>
<li>ç ”ç©¶è€…å‘ç°é¢„è®­ç»ƒå’Œå¾®è°ƒå¯ä»¥æé«˜å¤šä»»åŠ¡å­¦ä¹ çš„æ€§èƒ½</li>
<li>æ— ç›‘ç£é¢„è®­ç»ƒå¯ä»¥æ”¹è¿›seq2seqæ¨¡å‹</li>
<li><strong>æ›´å¤§çš„æ•°æ®é›†ã€æ›´åŠ å¼ºå¤§çš„ç»“æ„ã€æ–°çš„é¢„è®­ç»ƒä»»åŠ¡</strong></li>
<li>BiLMã€ELMoçš„è¾“å‡ºå¯ä»¥å¤§å¹…åº¦æ”¹å–„NLPä»»åŠ¡ï¼Œé€šå¸¸è¢«ç”¨æ¥è®¡ç®—ä¸Šä¸‹æ–‡è¯åµŒå…¥</li>
<li>ULMFiTçš„ä¸‰ä¸ªé˜¶æ®µï¼š
<ul>
<li>åœ¨é€šç”¨é¢†åŸŸä¸Šé¢„è®­ç»ƒLM</li>
<li>åœ¨ç›®æ ‡æ•°æ®ä¸Šå¾®è°ƒ</li>
<li>åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ</li>
</ul>
</li>
<li>è‡ªç›‘ç£ä»»åŠ¡ç”¨äºé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸Šå­¦ä¹ åˆ°æ›´å¤šçš„çŸ¥è¯†</li>
<li>å¾®è°ƒæ˜¯é¢„è®­ç»ƒä»»åŠ¡ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ä¸»è¦æ–¹å¼</li>
</ul>
</li>
</ul>
<h1 id="overview">Overview</h1>
<p>é¢„è®­ç»ƒæ¨¡å‹ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼šä¸Šä¸‹æ–‡ç¼–ç å™¨çš„ç”¨é€”ã€é¢„è®­ç»ƒä»»åŠ¡ã€ç›®çš„</p>
<h2 id="é¢„è®­ç»ƒä»»åŠ¡">é¢„è®­ç»ƒä»»åŠ¡</h2>
<ul>
<li>åˆ†ç±»
<ul>
<li>æœ‰ç›‘ç£
<ul>
<li>æŠŠè¾“å…¥æ˜ å°„åˆ°è¾“å‡ºä¸Š</li>
</ul>
</li>
<li>æ— ç›‘ç£
<ul>
<li>ä»æ— æ ‡æ³¨æ•°æ®ä¸­å¯»æ‰¾å†…åœ¨çš„è”ç³»ã€çŸ¥è¯†</li>
</ul>
</li>
<li>è‡ªç›‘ç£
<ul>
<li>ä¸Šé¢ä¸¤è€…çš„æ··åˆï¼Œå­¦ä¹ èŒƒå¼ä¸SLç›¸åŒï¼Œä½†æ˜¯å…¶è®­ç»ƒçš„æ ‡ç­¾æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„</li>
<li>æ ¸å¿ƒæ€æƒ³æ˜¯é¢„æµ‹è¾“å…¥çš„ä»»ä½•éƒ¨åˆ†ï¼Œç±»ä¼¼MLM</li>
</ul>
</li>
<li>NLPçš„æ•°æ®ä¸€èˆ¬æ¯”è¾ƒå°‘ï¼Œä¸è¶³ä»¥è®­ç»ƒå¥½çš„PTM</li>
<li>è¶Šæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡è¶Šå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰åˆ©ï¼Œä¾‹å¦‚MT</li>
</ul>
</li>
<li>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Loss%20Functions%20of%20Pre-training%20Tasks.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Loss%20Functions%20of%20Pre-training%20Tasks.pn)</a></li>
<li>LM
<ul>
<li>å•å‘LMçš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼šæ¯ä¸ªTokenåªç¼–ç äº†å·¦è¾¹çš„tokenå’Œä»–è‡ªå·±</li>
<li>BiLMæå‡ºtwo-toweræ¨¡å‹ï¼Œç¼–ç å‰å‘å’Œåå‘</li>
</ul>
</li>
<li>MLM
<ul>
<li>Masked LM
<ul>
<li>å®Œå½¢å¡«ç©ºï¼Œcloze</li>
<li>â€œcloze procedureâ€: A new tool for measuring readability</li>
<li>ç¼ºç‚¹ï¼š<strong>é€ æˆé¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µä¹‹é—´çš„æ–­å±‚ï¼Œå› ä¸º[mask]å¹¶ä¸å‡ºç°åœ¨å¾®è°ƒé˜¶æ®µ</strong></li>
<li>è§£å†³æ–¹æ¡ˆï¼šspecial token 80ï¼Œrandom 10ï¼Œorigin 10</li>
</ul>
</li>
<li>s2s MLM
<ul>
<li>MLMé€šå¸¸æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œsoftmaxåˆ†ç±»å™¨é¢„æµ‹</li>
<li>èŒƒå¼ï¼š<strong>ç¼–ç å™¨è¾“å…¥maskedåºåˆ—ï¼Œè§£ç å™¨åˆ©ç”¨è‡ªå›å½’é¡ºåºè¾“å‡ºmasked token</strong></li>
<li>MASSï¼ŒT5</li>
<li>ä»€ä¹ˆæ ·çš„é¢„è®­ç»ƒä»»åŠ¡åˆ©äºä»€ä¹ˆæ ·çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œs2s MLMæœ‰åˆ©äºqaã€summaryã€MT</li>
</ul>
</li>
<li>Enhanced Masked Language Modeling
<ul>
<li>RoBERTaï¼ŒåŠ¨æ€Mask</li>
<li>XLMï¼Œæ‹¼æ¥å¹³è¡ŒåŒè¯­å¥å¯¹ï¼ŒTranslation Language Modeling (TLM)</li>
<li>Span-BERTï¼Œå°†ç»“æ„ä¿¡æ¯ç¼–ç åˆ°é¢„è®­ç»ƒä¸­</li>
<li>å¤–éƒ¨çŸ¥è¯†</li>
</ul>
</li>
</ul>
</li>
<li>ä¹±åºLM
<ul>
<li>é¢„è®­ç»ƒæ—¶çš„maskåœ¨å¾®è°ƒé˜¶æ®µä¸å‡ºç°ï¼Œé€ æˆæ–­å±‚</li>
<li>è¾“å…¥å¥å­çš„tokenæ˜¯é‡æ–°æ’åºçš„</li>
<li>
<blockquote>
<p>In practice, only the last few tokens in the permuted sequences are predicted, due to the slow convergence. And a special two-stream self-attention is introduced for target-aware representations.</p>
</blockquote>
</li>
</ul>
</li>
<li>Denoising Autoencoder (DAE)
<ul>
<li>è¾“å…¥éƒ¨åˆ†æŸåï¼Œç›®çš„æ˜¯ä¸ºäº†å›å¤åŸæ¥çš„è¾“å…¥</li>
<li>ä¸‰ç§æ–¹å¼ï¼š
<ul>
<li>masking</li>
<li>åˆ é™¤ï¼Œåˆ¤æ–­ä¸¢å¤±éƒ¨åˆ†çš„ä½ç½®</li>
<li>å¡«å……ï¼Œé¢„æµ‹å¤šå°‘tokenä¸¢å¤±</li>
<li>Sentence Permutationï¼Œå¥å­ç½®æ¢ï¼Œ</li>
<li>Document Rotationï¼Œåˆ¤æ–­æ–‡æ¡£å¼€å§‹çš„ä½ç½®-å¥å­</li>
</ul>
</li>
</ul>
</li>
<li>å¯¹æ¯”å­¦ä¹ 
<ul>
<li>æ­£ä¾‹ä¹‹é—´åœ¨è¯­ä¹‰ä¸Šæ¯”éšæœºä¾‹å­æ›´ç›¸ä¼¼</li>
<li>é€šè¿‡å¯¹æ¯”æ¥å­¦ä¹ ä¸€èˆ¬æ€§çŸ¥è¯†</li>
<li>ä¸€èˆ¬è®¡ç®—å¤æ‚åº¦ç›¸æ¯”LMä½</li>
<li>taskï¼š
<ul>
<li>Deep InfoMax (DIM)
<ul>
<li>æœ€å…ˆç”¨äºå›¾åƒï¼Œé€šè¿‡æœ€å¤§åŒ–å›¾åƒè¡¨ç¤ºå’Œæœ¬åœ°åŒºåŸŸå›¾åƒä¹‹é—´çš„å…±åŒä¿¡æ¯æ¥æé«˜ptmçš„è´¨é‡</li>
<li>NLPä¸­ï¼Œä½¿n-gram spanæ¯”éšæœºçš„spanä¸maskedå¥å­è”ç³»æ›´é«˜</li>
</ul>
</li>
<li>Replaced Token Detection (RTD)
<ul>
<li>é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹tokenæ˜¯å¦è¢«æ›¿æ¢</li>
<li>[[Clark et al_2020_ELECTRA_arXiv2003.10555 [cs]|ELECTRA]]ä½¿ç”¨ç”Ÿæˆå™¨æ¥æ›¿æ¢åºåˆ—ä¸­çš„tokenï¼Œåˆ¤åˆ«å™¨åˆ¤æ–­å“ªäº›tokenèƒ½è¢«æ›¿æ¢äº†ï¼Œæ”¹è¿›äº†RTD</li>
<li>WKLMæ›¿æ¢å®ä½“ï¼Œè€Œä¸æ˜¯tokenï¼ŒæŠŠå®ä½“æ›¿æ¢æˆå…·æœ‰ç›¸åŒç±»å‹çš„å…¶ä»–å®ä½“ï¼Œä»è€Œå»é¢„æµ‹</li>
</ul>
</li>
<li>Next Sentence Prediction NSP
<ul>
<li>åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯ä¸æ˜¯æ¥è‡ªåŒä¸€ä¸ªè®­ç»ƒè¯­æ–™å¹¶ä¸”æ˜¯è¿ç»­çš„ï¼Œæ­£åä¾‹å‡ºç°çš„æ¯”ä¾‹æ˜¯ç›¸åŒçš„</li>
<li>å­¦ä¹ ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»</li>
<li>ä¸è¿‡NSPä»»åŠ¡ä¸å¤ªå¯é </li>
</ul>
</li>
<li>Sentence Order Prediction (SOP)
<ul>
<li>ç›®çš„æ˜¯ä½¿ç”Ÿæˆçš„å¥å­ã€æ–‡æ¡£æ›´å…·æœ‰ä¸€è‡´æ€§</li>
<li>ALBERTï¼Œsentence order prediction (SOP) loss</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ptmsåˆ†ç±»">PTMsåˆ†ç±»</h2>
<ul>
<li>å››ä¸ªæ–¹å‘
<ul>
<li>è¡¨ç¤ºç±»å‹ï¼šä¸Šä¸‹æ–‡</li>
<li>ç»“æ„ï¼šLSTMã€transformerã€encã€dec</li>
<li>é¢„è®­ç»ƒä»»åŠ¡ç±»å‹ï¼š[[Qiu et al_2020_Pre-trained Models for Natural Language Processing#é¢„è®­ç»ƒä»»åŠ¡|é¢„è®­ç»ƒä»»åŠ¡]]</li>
<li>æ‹“å±•ï¼šçŸ¥è¯†ã€å¤šè¯­è¨€ã€å¤šæ¨¡æ€ã€ç‰¹å®šè¯­è¨€ã€ç‰¹å®šé¢†åŸŸã€å‹ç¼©</li>
</ul>
</li>
<li>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Taxonomy%20of%20PTMs%20with%20Representative%20Examples.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Taxonomy%20of%20PTMs%20with%20Representative%20Examples.pn)</a></li>
<li>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/List%20of%20Representative%20PTMs.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/List%20of%20Representative%20PTMs.pn)</a></li>
</ul>
<h1 id="ptmsæ‹“å±•">PTMsæ‹“å±•</h1>
<h2 id="çŸ¥è¯†">çŸ¥è¯†</h2>
<ul>
<li>æ³¨å…¥çŸ¥è¯†ä¼šå—åˆ°ç¾éš¾æ€§é—å¿˜é—®é¢˜çš„å½±å“
<ul>
<li>K-Adapteråœ¨ä¸åŒçš„é¢„è®­ç»ƒä½¿ç”¨ä¸åŒçš„é€‚é…å™¨è¿›è¡Œæ³¨å…¥çŸ¥è¯†</li>
<li>K-BERTæ³¨å…¥å¤–éƒ¨çŸ¥è¯†ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œåœ¨å¾®è°ƒæ—¶æ³¨å…¥</li>
<li>Logan IV et al. [119] and Hayashi et al. [57] extended language model to knowledge graph language model (KGLM) and latent relation language model (LRLM)</li>
</ul>
</li>
</ul>
<h2 id="å¤šè¯­è¨€å’Œç‰¹å®šè¯­è¨€">å¤šè¯­è¨€å’Œç‰¹å®šè¯­è¨€</h2>
<ul>
<li>Cross-Lingual Language Understanding (XLU)
<ul>
<li><strong>é€šå¸¸éœ€è¦è¯­è¨€é—´çš„å¯¹é½</strong></li>
<li>mBertå¤šè¯­è¨€ä¸€èˆ¬åŒ–æ•ˆæœè¿˜å¯ä»¥</li>
<li>XLMç›¸æ¯”mbertå¼•å…¥äº†ä¸€ä¸ªå¤šè¯­è¨€ä»»åŠ¡ï¼Œtranslation language modeling (TLM)ï¼Œå¹³è¡ŒåŒè¯­è¯­æ–™èåˆ</li>
</ul>
</li>
<li>Cross-Lingual Language Generation (XLG)
<ul>
<li>XNLG [19] performs two-stage pre-training for cross-lingual natural language generation.
<ul>
<li>é¢„è®­ç»ƒå•è¯­å’Œå¤šè¯­MLM ç¼–ç å™¨</li>
<li>å›ºå®šç¼–ç å™¨ï¼Œè®­ç»ƒå•è¯­DAEå’Œäº¤å‰è¯­è¨€è‡ªåŠ¨ç¼–ç çš„è§£ç å™¨</li>
</ul>
</li>
<li>åœ¨äº¤å‰è¯­è¨€çš„é—®é¢˜ç”Ÿæˆå’Œæ‘˜è¦æ€»ç»“æ•ˆæœå¥½</li>
</ul>
</li>
</ul>
<h2 id="å¤šæ¨¡æ€">å¤šæ¨¡æ€</h2>
<blockquote>
<p>Typically, tasks like visual-based MLM, masked visual-feature modeling and visual-linguistic matching are widely used in multi-modal pre-training, such as VideoBERT [165], VisualBERT [103], ViLBERT [120].</p>
</blockquote>
<ul>
<li>åˆ†åˆ«å¯¹å›¾åƒå’Œæ–‡å­—è¿›è¡Œç¼–ç ï¼šViLBERT [120] and LXMERT [175]</li>
<li>åªæœ‰ä¸€ä¸ªtransformerï¼šVisualBERT [103], B2T2 [2], VLBERT [163], Unicoder-VL [101] and UNITER [17]</li>
</ul>
<h2 id="å‹ç¼©">å‹ç¼©</h2>
<ul>
<li>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Comparison%20of%20Compressed%20PTMs.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Comparison%20of%20Compressed%20PTMs.pn)</a></li>
<li>PTMsé€šå¸¸å‚æ•°é‡éƒ½ç›¸å½“å·¨å¤§ï¼Œå¾ˆéš¾éƒ¨ç½²åˆ°åœ¨çº¿æœåŠ¡å’Œç§»åŠ¨è®¾å¤‡ä¸Š</li>
<li>å‹ç¼©æ–¹æ³•ï¼š
<ul>
<li>æ¨¡å‹å‰ªæ
<ul>
<li>
<blockquote>
<p>Model pruning refers to removing part of neural network (e.g., weights, neurons, layers, channels, attention heads),</p>
</blockquote>
</li>
</ul>
</li>
<li>æƒå€¼é‡åŒ–
<ul>
<li>å°†é«˜ç²¾åº¦å‹ç¼©ä¸ºä½ç²¾åº¦</li>
</ul>
</li>
<li>å‚æ•°å…±äº«
<ul>
<li>è·¨å±‚ä¹‹é—´è¿›è¡Œå‚æ•°å…±äº«</li>
<li>ALBERTï¼Œå°½ç®¡è¿›è¡Œå…±äº«äº†ï¼Œä½†æ˜¯è®­ç»ƒå’Œæ¨æ–­çš„æ—¶é—´å˜é•¿äº†</li>
<li>åœ¨æ¨æ–­é˜¶æ®µå¹¶ä¸ä¼šæé«˜è®¡ç®—æ•ˆç‡</li>
</ul>
</li>
<li>çŸ¥è¯†è’¸é¦ï¼šTinyBERT</li>
<li>æ¨¡å—æ›¿æ¢ï¼šTheseus Compression</li>
</ul>
</li>
</ul>
<h1 id="åº”ç”¨ptmsåˆ°ä¸‹æ¸¸ä»»åŠ¡">åº”ç”¨PTMsåˆ°ä¸‹æ¸¸ä»»åŠ¡</h1>
<h2 id="è¿ç§»å­¦ä¹ ">è¿ç§»å­¦ä¹ </h2>
<ul>
<li>å¦‚ä½•è¿ç§»
<ul>
<li>é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒä»»åŠ¡ã€ç»“æ„ã€è¯­æ–™åº“
<ul>
<li>é¢„è®­ç»ƒä»»åŠ¡ã€ç»“æ„ã€è¯­æ–™åº“å¯¹äºä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ç›¸å½“å¤§</li>
<li>ä¾‹å¦‚ï¼šNSP-&gt;Question Answering (QA) and Natural Language Inference (NLI)</li>
<li>bertå¯ä»¥å¸®åŠ©ç†è§£ä»»åŠ¡ï¼Œä½†æ˜¯å¾ˆéš¾åšç”Ÿæˆä»»åŠ¡</li>
</ul>
</li>
<li>åˆé€‚çš„å±‚
<ul>
<li>é¢„è®­ç»ƒæ·±åº¦æ¨¡å‹æ¯å±‚æ‰€æ•æ‰çš„ä¿¡æ¯æ—¶ä¸åŒçš„ï¼Œ POS tagging, parsing,long-term dependencies, semantic roles, coreference</li>
<li>é€šå¸¸å¥æ³•ä¿¡æ¯åœ¨æµ…å±‚ï¼Œè¯­ä¹‰ä¿¡æ¯åœ¨æ·±å±‚</li>
<li>æ‰€ä»¥åœ¨ä½¿ç”¨ptmsçš„æ—¶å€™ï¼Œä½¿ç”¨é‚£ä¸€å±‚çš„ä¿¡æ¯ä¹Ÿæ˜¯å¾ˆé‡è¦çš„</li>
</ul>
</li>
<li>å¾®è°ƒï¼Ÿ
<ul>
<li>PTMsä¸€èˆ¬ä½œä¸ºç‰¹å¾æŠ½å–å™¨å’Œå¾®è°ƒï¼ŒåŒºåˆ«å°±æ˜¯çœ‹å‚æ•°å›ºä¸å›ºå®š</li>
<li>å¾®è°ƒå¯¹äºå„ç§ä»»åŠ¡æ¥è¯´æ›´åŠ æ–¹ä¾¿ä½¿ç”¨</li>
</ul>
</li>
<li>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Some%20common%20combinations%20of%20adapting%20PTMs.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Some%20common%20combinations%20of%20adapting%20PTMs.pn)</a></li>
</ul>
</li>
<li>å¾®è°ƒç­–ç•¥
<ul>
<li>ä¸¤é˜¶æ®µ
<ul>
<li>ä¸­é—´ä»»åŠ¡æˆ–è€…è¯­æ–™</li>
<li>ç›®æ ‡ä»»åŠ¡</li>
</ul>
</li>
<li>å¤šä»»åŠ¡å­¦ä¹ </li>
<li>é¢å¤–çš„æ¨¡å—
<ul>
<li>æ•ˆç‡ä½ä¸‹</li>
<li>æ”¹è¿›ï¼šåŸå§‹çš„å‚æ•°å›ºå®šçš„åŸºç¡€ä¸Šå¢åŠ ä¸€äº›å¾®è°ƒæ¨¡å—</li>
</ul>
</li>
<li>æ¨¡å‹èåˆå’Œè’¸é¦ç­‰</li>
</ul>
</li>
</ul>
<h1 id="åº”ç”¨">åº”ç”¨</h1>
<p>![](<a href="https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Resources%20of%20PTMs.pn)">https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/(https://cdn.jsdelivr.net/gh/imlauzh/img_host@master/research/Resources%20of%20PTMs.pn)</a></p>
<h2 id="general-evaluation-benchmark">General Evaluation Benchmark</h2>
<ul>
<li>The General Language Understanding Evaluation (GLUE) benchmark
<ul>
<li>
<blockquote>
<p>a collection of nine natural language understanding tasks, including single-sentence classification tasks (CoLA and SST-2), pairwise text classification tasks (MNLI, RTE, WNLI, QQP, and MRPC), text similarity task (STSB), and relevant ranking task (QNLI). GLUE benchmark is well-designed for evaluating the robustness as well as generalization of models.</p>
</blockquote>
</li>
<li>ä¸æä¾›çœŸå®æ ‡ç­¾ï¼Œè€Œæ˜¯ä¸€ä¸ªæœåŠ¡å™¨</li>
<li>superGLUE</li>
</ul>
</li>
<li>Question Answering
<ul>
<li><strong>æŠ½å–å¼QA-&gt;span prediction</strong></li>
<li><strong>multi-round generative QA</strong>ï¼šPTM+Adversarial Training+Rationale Tagging+Knowledge Distillation</li>
</ul>
</li>
<li>Sentiment Analysis</li>
<li>Named Entity Recognition
<ul>
<li>é¢„æµ‹æ¯ä¸ªtokenå¯¹åº”çš„label</li>
<li>bioå’Œbieso</li>
</ul>
</li>
<li>Machine Translation
<ul>
<li>MASSä½¿ç”¨seq2seq MLMå»ä¸€èµ·è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨</li>
</ul>
</li>
<li>Summarization
<ul>
<li>Zhongåˆ©ç”¨Siamese-BERTå»è®¡ç®—åŸæ–‡æ¡£ä¸æ€»ç»“ä¹‹é—´çš„ç›¸ä¼¼åº¦</li>
<li>Extractive summarization as text matching</li>
</ul>
</li>
<li>Adversarial Attacks and Defenses
<ul>
<li>é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“è„†å¼±</li>
<li>æ‰€ä»¥å¦‚ä½•æ”»å‡»å’Œé˜²å¾¡ä¹Ÿæ˜¯ç ”ç©¶çš„é‡ç‚¹</li>
<li>æ”»å‡»ï¼šåˆ©ç”¨äººç±»æ„ŸçŸ¥ä¸åˆ°çš„è¾“å…¥å˜åŒ–ä½†æ˜¯èƒ½å¤Ÿå½±å“æ¨¡å‹çš„è¾“å‡ºï¼Œè¯­æ³•ä¸Šæµç•…ï¼Œè¯­ä¹‰ä¸Šä¹Ÿè¦è¿è´¯</li>
<li>BERTåœ¨æ‹¼å†™é”™è¯¯ä¸Šä¸æ•æ„Ÿ</li>
</ul>
</li>
</ul>
<h1 id="æœªæ¥çš„æ–¹å‘">æœªæ¥çš„æ–¹å‘</h1>
<h2 id="ä¸Šé™">ä¸Šé™</h2>
<ul>
<li>PTMsè¿˜è¿œæ²¡æœ‰è¾¾åˆ°ä¸Šé™ï¼Œç°åœ¨å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯åˆ©ç”¨æ›´å¤šçš„è®­ç»ƒå’Œæ›´å¤§çš„æ•°æ®é›†å»æé«˜æ€§èƒ½</li>
<li>ä½†æ˜¯æˆ‘ä»¬éœ€è¦è€ƒè™‘çš„æ˜¯ï¼šæ›´åŠ åˆç†å’Œæ·±çš„æ¨¡å‹ç»“æ„ä»¥åŠæ›´æœ‰é’ˆå¯¹æ€§ã€æŒ‘æˆ˜æ€§çš„é¢„è®­ç»ƒä»»åŠ¡</li>
<li>
<blockquote>
<p>Therefore, a more practical direction is to design more efficient model architecture, self-supervised pre-training tasks, optimizers, and training skills using existing hardware and software.</p>
</blockquote>
</li>
<li>[[Clark et al_2020_ELECTRA_arXiv2003.10555 [cs]|ELECTRA]]</li>
</ul>
<h2 id="ç»“æ„">ç»“æ„</h2>
<ul>
<li>Transformerç»“æ„çš„ä¸»è¦å±€é™æ˜¯è®¡ç®—å¤æ‚åº¦ï¼Œä¸€èˆ¬æ˜¯è¾“å‡ºé•¿åº¦çš„å¹³æ–¹</li>
<li>ç›®å‰ç®—åŠ›ä¸å¤Ÿå‘è¾¾ï¼Œå¤§éƒ¨åˆ†æ¨¡å‹æœ€å¤šåªèƒ½å¤„ç†512ä¸ªtoken</li>
<li>è®¾è®¡æ·±åº¦æ¨¡å‹çš„ç»“æ„ç›¸å½“å›°éš¾ï¼Œä¹Ÿæœ‰è‡ªåŠ¨æœç´¢ç½‘ç»œç»“æ„çš„å·¥ä½œ [[Neural architecture search with reinforcement learning]]</li>
</ul>
<h2 id="ä»»åŠ¡å¯¼å‘å’Œæ¨¡å‹å‹ç¼©">ä»»åŠ¡å¯¼å‘å’Œæ¨¡å‹å‹ç¼©</h2>
<ul>
<li>PTMså’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„çŸ›ç›¾ä¸»è¦æœ‰ä¸¤æ–¹é¢ï¼š
<ul>
<li>æ¨¡å‹æ¶æ„</li>
<li>æ•°æ®åˆ†å¸ƒ</li>
</ul>
</li>
<li>å‚æ•°é‡å¤§ï¼Œç§»æ¤åˆ°ç§»åŠ¨è®¾å¤‡ç­‰-&gt;å‹ç¼©</li>
</ul>
<h2 id="knowledge-transfer-beyond-fine-tuning">Knowledge Transfer Beyond Fine-tuning</h2>
<ul>
<li>å›ºå®šåŸå§‹å‚æ•°,åœ¨è¿™åŸºç¡€ä¸Šå¢åŠ ä¸€äº›å¯å¾®è°ƒçš„æ¨¡å—
<ul>
<li>BERT and PALs: Projected attention layers for ecient adaptation in multi-task learning</li>
<li>Parameter-efficient transfer learning for NLP</li>
</ul>
</li>
<li>éœ€è¦æ›´æœ‰æ•ˆç‡çš„æ¨¡å‹</li>
</ul>
<h2 id="interpretability-and-reliability-of-ptms">Interpretability and Reliability of PTMs</h2>


  
  <footer>
    <hr>
    
    <div class="post-tags">
    
    </div>
  </footer>
  

  


  <div class="comments">



  <div class="comments-item" >
    
    
    
    <div id="vcomments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <script type="text/javascript">
      new Valine({
          el: '#vcomments',
          highlight: false,
          lang: "en",
          appId: "O9aoAtFO2Mk0VrPqbyHMHwah-gzGzoHsz",
          appKey: "1bF6m0SPiN3sk9TaGxPELdjY",
          placeholder: "Say Something......",
          requiredFields: ["nick","mail"],
          avatar: "robohash",
          visitor:  true ,
          recordIP: true
      });
    </script>
    <script>
      if(window.location.hash){
          var checkExist = setInterval(function() {
             if ($(window.location.hash).length) {
                $('html, body, article').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
                clearInterval(checkExist);
             }
          }, 100);
      }
    </script>
  </div>

</div>

</article>



  
  
  
</body>
<div class="foot">
  
  
    &copy; 2017 - 2020 &#183; 
    <a href="/">imlauzh</a> Â· Theme <a href="https://github.com/RainerChiang/simpleness">Simpleness</a> Powered by <a href="https://gohugo.io/">Hugo</a> &#183;
    <a href="#"><i class="fas fa-chevron-up"></i></a>
  

  
</div>

<script src="/blog/js/lazyload.min.js"></script>
<script>
  var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });
</script>


</html>
