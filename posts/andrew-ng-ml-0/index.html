<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.81.0" />

  <title> 吴恩达「机器学习」（一） |  imlauzh</title>
  <meta name="description" content="A website built by Joseph Lau and host by Github pages.">
  <link rel="stylesheet" href="/blog/css/index.css">
  <link rel="stylesheet" href="/blog/css/classes.css">
  <link rel="canonical" href="/blog/posts/andrew-ng-ml-0/">
  <link rel="alternate" type="application/rss+xml" href="" title="imlauzh">
  
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">
  <link 
    rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" 
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" 
    crossorigin="anonymous">
  </link>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" 
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" 
    crossorigin="anonymous">
  </script>
  <script 
    defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" 
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" 
    crossorigin="anonymous" 
    onload="renderMathInElement(document.body);">
  </script>
</head>

<body>
  <header class="menus">
  

  <nav >
    
    <a href="/blog/"> Home</a>
    
    <a href="/blog/categories/"> Categories</a>
    
    <a href="/blog/tags/"> Tags</a>
    
    <a href="/blog/about/"> About</a>
    
    <a href="/blog/index.xml"> Subscribe</a>
    
  </nav>

  <nav class="fontawesome">
    
    <a href="https://github.com/imlauzh" target="_blank">
        <i title="GitHub" class="fab fa-github"></i>
    </a>
    
    
    <a href="/blog/index.xml" target="_blank">
        <i title="RSS" class="fas fa-rss"></i>
    </a>
    
  </nav>
  
  
  <div class="hidden description">A website built by Joseph Lau and host by Github pages.</div>
  
</header>

<article id="article">
  <header>
  
    <i class="fas fa-folder"></i>
    
    <a href="/blog/categories/machine-learning">Machine Learning</a>
    &nbsp;
    
  

    <h1 style="text-align: center;" >吴恩达「机器学习」（一）</h1>
    <div class="post-meta">
    
      <time datetime="2019-04-05T17:03:30Z">April 05, 2019</time> &nbsp; 
    

     &nbsp;

    
    
      <i class="far fa-eye"></i>
      <span id="/blog/posts/andrew-ng-ml-0/" class="leancloud_visitors" data-flag-title="吴恩达「机器学习」（一）">
          <span class="leancloud-visitors-count">  </span>
      </span> &nbsp;
    
    

    
      <i class="far fa-clock"></i>
      
      
      

      
        30 min
      
      33 s
      &nbsp;
    
    </div>
  </header>

  <ul>
<li>什么是机器学习？</li>
<li>什么是监督学习算法？</li>
<li>什么是非监督学习算法？</li>
<li>什么是单变量线性回归？</li>
<li>线性代数知识点回顾</li>
</ul>
<h2 id="什么是机器学习">什么是机器学习？</h2>
<ul>
<li>
<p>Arthur Samuel(1959)</p>
<p>编程使计算机自己和自己玩10^1000跳棋游戏</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc834f312e50.jpg" alt=""></p>
<ul>
<li>
<p>Tom Mitchell(1998)</p>
<p>计算机程序从经验E中学习任务T，并用度量P来衡量性能。条件是它由P定义的关于T的性能随着经验E而提高。</p>
<p>经验E=自己玩10^1000次跳棋</p>
<p>任务T=玩跳棋</p>
<p>性能度量P=与<strong>新</strong>对手玩跳棋时赢的概率</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc8323ea501d.jpg" alt="image-20190406171015837"></p>
<ul>
<li>邮件系统学习标记垃圾邮件</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc83246c6fb6.jpg" alt="image-20190406171657536"></p>
<ul>
<li>
<p>学习算法</p>
<ul>
<li>supervised learning
<ul>
<li>我们教计算机如何做一件事情</li>
<li>给标准答案</li>
</ul>
</li>
<li>unsupervised learning
<ul>
<li>计算机自己学习</li>
</ul>
</li>
<li>others
<ul>
<li>reinforcement learning</li>
<li>recommender systems</li>
</ul>
</li>
</ul>
</li>
<li>
<p>对应用学习算法的实用性建议（木匠）</p>
<p>如何去使用这些工具（机器学习算法）</p>
</li>
</ul>
<h2 id="监督学习">监督学习</h2>
<h3 id="example">Example</h3>
<ul>
<li>
<p>predict housing prices</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8324f7daff.jpg" alt="image-20190406201909053"></p>
<ul>
<li>直线拟合</li>
<li>二次函数或者二次多项式</li>
</ul>
<p>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做<strong>回归问题</strong>。我们试着推测出一个连续值的结果，即房子的价格。</p>
<p>一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。</p>
</li>
<li>
<p>breast cancer</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8325695ce7.jpg" alt="image-20190406202851738"></p>
<p>​	Malignant or benign？-&gt;(0,1)</p>
<p>​	这类机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。</p>
<p>​	分类指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。0 代表良性，这也是分类问题，属于多分类问题。可以用另外一种形式来表示（O表示malignant，X表示bengin）。</p>
<ul>
<li>
<p>多特征输入</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8326024fa1.jpg" alt="image-20190406203620654"></p>
<p>例如年龄、tumor大小、malignant or bengin……</p>
<p>良性区域or恶性区域</p>
<p>如何处理这些无穷多数量的特征？如何存储？你电脑的内存肯定不够用。**我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。**想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征，事实上，我们能用算法来处理它们。</p>
</li>
</ul>
</li>
</ul>
<h3 id="测验">测验</h3>
<p>现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题：</p>
<ol>
<li>你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你想预测接下来的三个月能卖多少件？</li>
<li>你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？</li>
</ol>
<p>那这两个问题，它们属于分类问题、还是回归问题?</p>
<p>问题一是一个回归问题，因为你知道，如果我有数千件货物，我会把它看成一个实数，一个连续的值。因此卖出的物品数，也是一个连续的值。</p>
<p>问题二是一个分类问题，因为我会把预测的值，<strong>用 0 来表示账户未被盗，用 1 表示账户曾经被盗过</strong>。所以我们根据账号是否被盗过，把它们定为0 或 1，然后用算法推测一个账号是 0 还是 1，因为只有少数的离散值，所以我把它归为分类问题。</p>
<p>以上就是监督学习的内容。</p>
<h3 id="总结">总结</h3>
<p>这节课我们介绍了监督学习。其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。</p>
<h2 id="无监督学习">无监督学习</h2>
<p><img src="https://i.loli.net/2019/04/30/5cc83263b63bf.jpg" alt="image-20190406205004602"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc83268ac8aa.jpg" alt="image-20190406205023673"></p>
<p>在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？</p>
<p>针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。</p>
<ul>
<li>聚类应用</li>
</ul>
<p><img src="https://i.loli.net/2019/04/30/5cc8326e8d92a.jpg" alt="image-20190406205247834"></p>
<p>谷歌新闻每天都在，收集非常多，非常多的网络的新闻内容。它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件，自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8327321c37.jpg" alt="image-20190406205624859"></p>
<ul>
<li>
<p>Cocktail party problem</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832792a760.jpg" alt="image-20190406210053078"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc8327cf19b3.jpg" alt="image-20190406210112864"></p>
</li>
</ul>
<h2 id="单变量线性回归">单变量线性回归</h2>
<p>Linear Regression with One Variable</p>
<h3 id="模型表示">模型表示</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc83283692bf.jpg" alt="image-20190406211745719"></p>
<p><img src="https://i.loli.net/2019/04/30/5cc83288e5014.jpg" alt="image-20190406212146531"></p>
<p>一种可能的表达方式为：</p>
<p>$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$</p>
<p>因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
<h3 id="代价函数">代价函数</h3>
<p>我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8328faf434.jpg" alt="image-20190406214055225"></p>
<p>在线性回归中我们有一个像这样的训练集，$m$代表了训练样本的数量，比如 $m = 47$。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：$h_\theta \left( x \right)=\theta_{0}+\theta_{1}x$。</p>
<p>接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的<strong>参数</strong>（<strong>parameters</strong>）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率和在$y$ 轴上的截距。</p>
<p>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差</strong>（<strong>modeling error</strong>）。</p>
<p>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数</p>
<p>$$
J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}
$$</p>
<p>最小。(平方误差函数)</p>
<p>还有其他代价函数，不过平方误差函数对于线性回归是比较合理的。</p>
<p>我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：</p>
<p><img src="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/raw/master/images/27ee0db04705fb20fab4574bb03064ab.png" alt="img"></p>
<h3 id="代价函数的直观理解一">代价函数的直观理解（一）</h3>
<p>在上一个视频中，我们给了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8329450cff.jpg" alt=""></p>
<p>简化函数，先假设截距为0，计算简单的情况。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8329ad7de3.jpg" alt=""></p>
<p>当$\theta_1$改变时（$\theta_1=0,0.5,1,1.5,2,2.5&hellip;$），计算$J(\theta_1)$的函数图像。对于每一个$\theta_1$都有一个对应的$J(\theta_1)$，那么我们就可以画出来$J(\theta_1)$的函数图像，也就是代价函数的函数图像。为了找到一个最拟合数据的函数，需要改变$\theta_1$的值，从而使代价函数最小，也就是当$\theta_1=1$时可以拟合得到出((1,1),(2,2),(3,3))的函数。</p>
<h3 id="代价函数的直观理解二">代价函数的直观理解（二）</h3>
<p>前提：熟悉等高线图的绘制</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832a1a86c0.jpg" alt=""></p>
<p>代价函数的样子，<strong>等高线图可以更好的体现出函数图像上的点之间的关系</strong>，则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点（*碗底*）。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832a82a742.jpg" alt=""></p>
<p>通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。</p>
<p>当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。</p>
<p>我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值。</p>
<h3 id="梯度下降">梯度下降</h3>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\left( \theta_0,\theta_1,&hellip;&hellip;,\theta_n \right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>
<p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_{0}, \theta_{1})$ 的最小值。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832b389486.jpg" alt=""></p>
<p>对$\theta_0$,$\theta_1$进行一些初步猜想，他们是多少并不重要，将他们全部设为0，我们不停的一点点改变$\theta_0$,$\theta_1$试图使$J(\theta_{0}, \theta_{1})$最小或者使局部最小值。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832c0a103a.jpg" alt=""></p>
<p><img src="https://i.loli.net/2019/04/30/5cc832ca74768.jpg" alt=""></p>
<p>想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p><strong>不同的起点，最终获得的局部最低点可能是不同的</strong>。</p>
<p>批量梯度下降（<strong>batch gradient descent</strong>）算法的公式为：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832ce153c2.jpg" alt=""></p>
<p>我们使用$:=$表示赋值，$=$表示判断为真的声明。</p>
<p>其中$\alpha$是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都<strong>同时</strong>让<strong>所有的参数减去学习速率乘以代价函数的导数</strong>，同时更新$\theta_0$,$\theta_1$。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832d48a96d.jpg" alt=""></p>
<p>在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新$\theta_0$和$\theta_1$ ，当 $j=0$ 和$j=1$时，会产生更新，所以你将更新$J\left( \theta_0 \right)$和$J\left( \theta_1 \right)$。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要<strong>同时更新$\theta_0$和$\theta_1$</strong>，我的意思是在这个等式中，我们要这样更新：</p>
<p>$\theta_0$:= $\theta_0$ ，并更新$\theta_1$:= $\theta_1$。</p>
<p>实现方法是：你应该计算公式右边的部分，通过那一部分计算出$\theta_0$和$\theta_1$的值，然后同时更新$\theta_0$和$\theta_1$。在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p>
<p>在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项：
$$
\alpha \frac{\partial }{\partial \theta_0}J(\theta_0,\theta_1)
$$</p>
<p>$$
\alpha \frac{\partial }{\partial \theta_1}J(\theta_0,\theta_1)
$$</p>
<p>如果你不熟悉微积分，不用担心，即使你之前没有看过微积分，或者没有接触过偏导数，在接下来的视频中，你会得到一切你需要知道，如何计算这个微分项的知识。</p>
<h3 id="梯度下降的直观理解">梯度下降的直观理解</h3>
<p><img src="https://i.loli.net/2019/04/30/5cc832d8987ff.jpg" alt="image-20190411163204029"></p>
<p>梯度下降就是为了找到代价函数梯度最小的位置，也就是极小值，每次都要使$\theta$越来越接近0。</p>
<p>$\alpha$作为学习率，是每次梯度下降的步长，太小容易导致梯度下降太慢，太大容易导致梯度下降错过最小值，可能无法收敛，甚至分散。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832dd6ab33.jpg" alt="image-20190411164605244"></p>
<p>随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数的改变会变小一点点。所以<strong>我们即使用固定的学习率$\alpha$，我们也可以最终收敛到局部最小值。</strong></p>
<p>在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$\alpha$。</p>
<p>这就是梯度下降算法，你可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。</p>
<h3 id="梯度下降的线性回归">梯度下降的线性回归</h3>
<p>在以前的视频中我们谈到关于梯度下降算法，梯度下降是很常用的算法。在这段视频中，我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832e211dd3.jpg" alt="image-20190411170353256">
$$
\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial \theta_j} \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\<br>
=\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial \theta_j} \frac{1}{2m}\sum\limits_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2
$$
也就是对于$j$取$0,1$时：
$$
j=0:\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})
$$</p>
<p>$$
j=1:\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
$$</p>
<p>我们可以把梯度下降算法写成下面这个样子：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc832e73a42e.jpg" alt="image-20190411172420834"></p>
<p>我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”<strong>批量梯度下降</strong>”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有$m$个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一&quot;批&quot;训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种&quot;批量&quot;型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。</p>
<p>如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数$J$最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数$J$的最小值，这是另一种称为正规方程(<strong>normal equations</strong>)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。</p>
<h3 id="whats-next">What’s next</h3>
<p>在接下来的一组视频中，我会对线性代数进行一个快速的复习回顾。</p>
<p>通过它们，你可以实现和使用更强大的线性回归模型。事实上，线性代数不仅仅在线性回归中应用广泛，它其中的矩阵和向量将有助于帮助我们实现之后更多的机器学习模型，并在计算上更有效率。正是因为这些矩阵和向量提供了一种有效的方式来组织大量的数据，特别是当我们处理巨大的训练集时，如果你不熟悉线性代数，如果你觉得线性代数看上去是一个复杂、可怕的概念，特别是对于之前从未接触过它的人，不必担心，事实上，为了实现机器学习算法，我们只需要一些非常非常基础的线性代数知识。通过接下来几个视频，你可以很快地学会所有你需要了解的线性代数知识。具体来说，为了帮助你判断是否有需要学习接下来的一组视频，我会讨论什么是矩阵和向量，谈谈如何加、减 、乘矩阵和向量，讨论逆矩阵和转置矩阵的概念。</p>
<h2 id="线性代数回顾linear-algebra-review">线性代数回顾(Linear Algebra Review)</h2>
<h3 id="矩阵和向量">矩阵和向量</h3>
<p>矩阵：二维数组、向量：一维数组</p>
<p>矩阵的维数即行数×列数</p>
<p>矩阵元素（矩阵项）：
$$
A=\left[\begin{matrix}1402 &amp; 191 \ 1371 &amp; 821 \ 949 &amp; 1437 \ 147 &amp; 1448\end{matrix}\right]
$$
$A_{ij}$指第$i$行，第$j$列的元素。</p>
<p>向量是一种特殊的矩阵，只有一列，讲义中的向量一般都是列向量，如： 
$$
y=\left[ \begin{matrix} 460 \ 232 \ 315 \ 178 \end{matrix} \right]
$$
为<strong>四维</strong>列向量（4×1）。</p>
<p>一般我们默认使用1索引：</p>
<p><img src="https://i.loli.net/2019/04/30/5cc8331283f50.jpg" alt="image-20190411225117460"></p>
<p>大写字母我们一般用来表示矩阵，例如矩阵$A$，小写字母用来表示数字。</p>
<h3 id="加法和标量乘法">加法和标量乘法</h3>
<p>矩阵的加法：行列数相等的可以加。</p>
<p>例如：
$$
\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]+\left[\begin{matrix}4&amp;0.5 \ 2&amp;5 \ 0&amp;1\end{matrix}\right]=\left[\begin{matrix}5&amp;0.5 \ 4&amp;10 \ 3&amp;2\end{matrix}\right]
$$
矩阵的乘法：每个元素都要乘</p>
<p>例如：
$$
3\times\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]=\left[\begin{matrix}3&amp;0 \ 6&amp;15 \ 9&amp;3\end{matrix}\right]=\left[\begin{matrix}1&amp;0 \ 2&amp;5 \ 3&amp;1\end{matrix}\right]\times3
$$</p>
<p>$$
\left[\begin{matrix}4&amp;0\6&amp;3\end{matrix}\right]/4=\frac{1}{4}\times\left[\begin{matrix}4&amp;0\6&amp;3\end{matrix}\right]=\left[\begin{matrix}1&amp;0\\frac{3}{2}&amp;\frac{3}{4}\end{matrix}\right]
$$</p>
<h3 id="矩阵向量乘法">矩阵向量乘法</h3>
<p>矩阵和向量的乘法如下：$m×n$的矩阵乘以$n×1$的向量，得到的是$m×1$的向量
$$
\left[\begin{matrix}1&amp;3\4&amp;0\2&amp;1\end{matrix}\right]\left[\begin{matrix}1\5\end{matrix}\right]=\left[\begin{matrix}16\4\7\end{matrix}\right]
$$</p>
<h3 id="矩阵乘法">矩阵乘法</h3>
<p>矩阵乘法：</p>
<p>$m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。</p>
<p><img src="https://i.loli.net/2019/04/30/5cc83318dd20a.jpg" alt="image-20190412182704551"></p>
<h3 id="矩阵乘法的性质">矩阵乘法的性质</h3>
<p>矩阵的乘法不满足交换律：$A×B≠B×A$</p>
<p>矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$</p>
<p>单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示，本讲义都用 $I$ 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0。如：</p>
<p>$AA^{-1}=A^{-1} A=I$</p>
<p>对于单位矩阵，有$A\cdot I=I\cdot A=A$</p>
<h3 id="矩阵逆转置">矩阵逆、转置</h3>
<p>矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$AA^{-1}=A^{-1}A=I$</p>
<p>矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$</p>
<p>定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记$A^T=B$。(有些书记为A'=B）</p>
<p>直观来看，将$A$的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到$A$的转置。</p>
<p>例：</p>
<p>$$
\left| \begin{matrix} a&amp; b \ c&amp; d \ e&amp; f \end{matrix} \right|^{T}=\left|\begin{matrix} a&amp; c &amp; e \ b&amp; d &amp; f \end{matrix} \right|
$$</p>
<p>矩阵的转置基本性质:</p>
<p>$$
\left( A\pm B \right)^{T}=A^T\pm B^T\ \left( A\times B \right)^{T}=B^{T}\times A^{T}\\left( A^T \right)^{T}=A \ \left( KA \right)^{T}=KA^{T}
$$
<strong>matlab</strong>中矩阵转置：直接打一撇，<code>x=y'</code>。</p>

  
  <footer>
    <hr>
    
    <div class="post-tags">
    
      <i class="fas fa-tags"></i>
      
        <a href="/blog/tags/andrew-ng">Andrew NG</a>
        &nbsp;
      
    
    </div>
  </footer>
  

  


  <div class="comments">



  <div class="comments-item" >
    
    
    
    <div id="vcomments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <script type="text/javascript">
      new Valine({
          el: '#vcomments',
          highlight: false,
          lang: "en",
          appId: "O9aoAtFO2Mk0VrPqbyHMHwah-gzGzoHsz",
          appKey: "1bF6m0SPiN3sk9TaGxPELdjY",
          placeholder: "Say Something......",
          requiredFields: ["nick","mail"],
          avatar: "robohash",
          visitor:  true ,
          recordIP: true
      });
    </script>
    <script>
      if(window.location.hash){
          var checkExist = setInterval(function() {
             if ($(window.location.hash).length) {
                $('html, body, article').animate({scrollTop: $(window.location.hash).offset().top-90}, 1000);
                clearInterval(checkExist);
             }
          }, 100);
      }
    </script>
  </div>

</div>

</article>



  
  
  
</body>
<div class="foot">
  
  
    &copy; 2017 - 2021 &#183; 
    <a href="/">imlauzh</a> · Theme <a href="https://github.com/RainerChiang/simpleness">Simpleness</a> Powered by <a href="https://gohugo.io/">Hugo</a> &#183;
    <a href="#"><i class="fas fa-chevron-up"></i></a>
  

  
</div>

<script src="/blog/js/lazyload.min.js"></script>
<script>
  var lazyImage = new LazyLoad({
    container: document.getElementById('article')
  });
</script>


</html>
