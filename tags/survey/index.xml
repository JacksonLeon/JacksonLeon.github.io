<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Survey on 早柳</title>
    <link>/tags/survey/</link>
    <description>Recent content in Survey on 早柳</description>
    <image>
      <url>/</url>
      <link>/</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 05 Dec 2020 15:57:21 +0800</lastBuildDate><atom:link href="/tags/survey/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文笔记 - Pre Trained Models for Natural Language Processing</title>
      <link>/posts/pretrain-survey/</link>
      <pubDate>Sat, 05 Dec 2020 15:57:21 +0800</pubDate>
      
      <guid>/posts/pretrain-survey/</guid>
      <description>大致框架  基于四个不同的方向给PTM分类 如何将PTM的知识应用到下游任务中去 未来的研究中PTM潜在的几个方向 本文旨在帮助研究者理解、使用和发展各种NLP任务下的PTM  介绍 现在NLP任务主要是应用神经网络模型，一个优势是避免了[[Feature engineering|特征工程]]问题。而非神经网络一般相当依赖离散的手动构建的特征。现如今神经网络都是使用低维度、密集的向量（[[Distributed Representation]]）来隐含地表示表示语言的句法或语法特征。
相比[Computer Vision]，NLP领域的进展就比较小了，主要是因为大多数监督NLP任务的数据集太小（机器翻译除外）,大数量的模型参数在小数据上训练经常产生过拟合的现象，所以早期的NLP模型大多是窄模型，且只有1-3层。
预训练模型在大语料库上可以学习到通用的语言表示，而且不用从头开始训练一个新模型。
第一代预训练模型主要目的是为了更好地学习词嵌入表示。计算效率上有些低，不考虑上下文信息，所以也就不能捕获更深层的特征信息，存在一些问题：polysemous disambiguation, syntactic tructures, semantic roles, anaphora。 主要代表：Skip-Gram [Distributed representations of words and phrases and their compositionality] and GloVe [GloVe: Global vectors for word representation].
第二代PTMs主要重点在于学习上下文词嵌入 例如：CoVe [Learned in translation: Contextualized word vectors], ELMo [Deep contextualized word representations], [[Radford et al_Improving Language Understanding by Generative Pre-Training|OpenAI GPT]] and [[Devlin et al_2019_BERT|BERT]]。下游NLP任务还是需要这些训练过的编码器来表示上下文的词。
详细框架  PTMs中的背景知识、常用的符号 PTMs一个简短的总结以及分类 PTMs的扩展 如何应用到下游任务 PTMs的相关资源 NLP任务的集合 现在的挑战以及未来的方向  背景知识 语言表示学习  a good representation should express general-purpose priors that are not task-specific but would be likely to be useful for a learning machine to solve AI-tasks.</description>
    </item>
    
    <item>
      <title>论文笔记 -  More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction</title>
      <link>/posts/more-data-re/</link>
      <pubDate>Mon, 02 Nov 2020 15:55:50 +0800</pubDate>
      
      <guid>/posts/more-data-re/</guid>
      <description>Introduction  These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. 现实情况下会更加复杂：  高质量人工标注数据代价高 关系抽取数据存在长尾现象 大部分的事实数据是出现在更大的上下文中，多个句子中 预先定义好的关系集合无法覆盖所有实际存在的关系   概括出四个可行的方向  利用更多的数据  [[Distant Supervision|远程监督]] 但是，DS带来标签错误问题，单个句子包含实体对 如何利用远程监督或者其他方法来获取高质量、大规模的数据去训练？   更有效率的学习 更加复杂的上下文  现在的大部分模型都是抽取单句内的关系，两句或更大的上下文还无法很好的利用   开放域 关键挑战  [[Peng et al_2020_Learning from Context or Names|learning from text or names]] datasets towards special interests      Background and ExistingWork  一个完整的关系抽取系统  命名实体识别 实体链接到知识图谱（如果使用KGs的话） 关系分类器     Pattern Extraction Models  自动构建模式规则 更大的数据集 更多模式格式 这部分工作大部分都需要人类专家的检验，也是主要的限制  Statistical Relation Extraction Models  覆盖更多的内容，基本不需要人类干预 feature-based methods  设计词汇的、句法、语义的特征   kernel-based methods  比较关系的表示和句子之间的相似度   Graphical methods  利用无环图来抽象实体、句子、关系之间的依赖，从而判断关系类型   embedding models  把文本编码成低维度的向量表示   challenges  需要人工设计特征和核函数 模型容量    Neural Relation Extraction Models  recursive neural networks，CNN，RNN，GNNs，attention-based [[Word Embedding|词嵌入]]  输入文本的语义表示   [[Positional Embeddings|位置嵌入]]  词语和实体之间的相对距离   预训练模型等  “More” Directions for RE Utilizing More Data  缺少高质量，大规模数据  远程监督和启发式方法可以产生大规模的数据 但是，不可避免带来噪音标签 所以降噪：  从multi-instance中选择最具有信息的 利用额外的上下文信息：KGs，多语言语料 利用复杂的机制和训练策略  强化学习重新打标签[[Chen et al_2020_Relabel the Noise]]     还存在问题：  目前的远程监督方法比较古老2009年，更好的远程监督机制 无监督或者半监督方法   Wiki-Distant  Performing More Efficient Learning  长尾现象，数据分布不均  [[Han et al_2018_FewRel]]FewRel数据集  N way k shot 去学习例子更好的表示   元学习  学习如何去学 参数初始化和优化   Few-shot领域迁移 Few-shot NOTA 评价标准，目前的Fewshot更偏向于拟合句子表示，而不是真正的学习语义  Handling More Complicated Context   需要阅读、记忆、推理 更多形式的上下文信息：对话、文档等等 利用搜索引擎获取外部知识  Orienting More Open Domains  开放信息抽取  从句子中抽取实体和关系    关系发现  从无监督数据中发现 聚类 生成   问题：  关系规范化 N/A类型，并不是每个实体对都存在关系    Other Challenges Learning from Text or Names  实体和上下文都提供了重要的信息   RE Datasets towards Special Interests  针对具体问题的数据集基本没有，例如跨句子的关系抽取数据集 构建结构清晰，设计优秀的细粒度特定问题的数据集是相当有必要的  </description>
    </item>
    
  </channel>
</rss>
