<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NLP-1 | GeekJoe</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Posts</a></li>
      
      <li><a href="/note/">Notes</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">NLP-1</span></h1>

<h2 class="date">2019/09/21</h2>
</div>

<main>
<h1 id="nlper-建议">NLPer 建议</h1>
<blockquote>
<p>转载自知乎专栏https://zhuanlan.zhihu.com/p/69358248</p>
<p>作者：<a href="https://www.zhihu.com/people/songyingxin">老宋的茶书会</a>，诗和远方，我选择苟且</p>
<p>前些日子刷知乎的时候，刷到一个研究生提到，其在开学之初，老师给了他一份礼物： <strong>论文大礼包</strong>。emmm， 瞬间觉得自己好惨，我整个研究生涯完全是放养，自己摸索过来的， 不要说论文大礼包，老师都没见过几次，哎，说多了都是泪。</p>
<p>考虑到， 我国研究生现状， 估计很多人都无法遇到很合意的老师，更不要说论文大礼包了，于是，我决定，将我整个一年多来看过的，觉得值得一看的论文总结一下，给各位还在挣扎在NLP 门口的各位同学一些参考。</p>
</blockquote>
<h2 id="先来谈谈学习建议">先来谈谈学习建议</h2>
<p>从我的学习道路来看， 中间的确走了很多弯路， 下面我来细数一下。</p>
<p><strong>机器学习：不要花太多精力</strong></p>
<p>前期花在机器学习上的时间也不算少，反反复复也滤了差不多三遍，但是，记忆丢失太快，很多东西随着时间流逝就只剩下概念和思想了，一些细节实在想不起来。 对此，我的建议是，对于 NLPer 来说，机器学习算法有一定的地位，但是重点不要放在那里，因为你用到的并不多，遗忘的速度要比记忆的速度快得多。</p>
<p><strong>Pytorch 完胜 TensorFlow</strong></p>
<p>Pytorch 对于研究者来说，其易用性要比 TensorFlow 强十倍不止， 我当初上手 TensorFlow 用了半个多月， 而Pytorch， 只用了半天。TensorFlow 太过复杂， API 太多，往往让你不知所措，强烈建议使用 Pytorch。</p>
<p><strong>文本分类 vs 阅读理解</strong></p>
<p>对于初学者来说，最初选择一个相对简单的方向是最佳的，文本分类就是一个很好的选择，不仅仅是因为其简单，更是因为其在工业界也是用的最多的。</p>
<p>想当初，我直接上手 <strong>阅读理解</strong>，阅读理解是一个很棒的方向，但模型实在太复杂了，对于初学者来说相当不友好，如果你还使用TensorFlow，恭喜你，入门看 Paper 就得至少俩月，那段时间真的是欲生欲死。（没错，我看的第一篇Paper 就是 BiDAF， 那个时候我连 Word Embedding 还没搞清楚，结果就被虐的很惨）。</p>
<p>对于文本分类，可以看看我之前的文章：<a href="https://zhuanlan.zhihu.com/p/64603089">几个可作为Baseline的文本分类模型</a>， 从这篇文章中涉及到的东西延伸， 一个月就足以完全摸透文本分类领域了，此时你就算真的入门了。顺便推荐一下我的仓库：<strong>TextClassification-Pytorch</strong></p>
<p><strong>代码和 Paper 同等重要</strong></p>
<p>这个就不多说了，如果你非说，我是搞理论的，麻烦出门右转。 实现一些经典的 Paper 对自己是很有帮助的。</p>
<p><strong>不要随随便便答应你的学姐学长</strong></p>
<p>有时候，你的学姐学长会想让你帮他实现一下论文或其他， 请仔细考虑，如果你的师姐，师兄很厉害（代码牛逼或论文顶会），那你当然屁颠屁颠的帮忙去写，而大多数情况是， 你的师兄师姐，不强，潜台词我就不说了， 不要轻易浪费你的时间，学会拒绝。</p>
<p><strong>如果可以，早点出去实习</strong></p>
<p>现在很多公司的资源要比实验室丰富的多，大牛也多，早点实习开阔眼界，我现在就挺后悔自己当初懵懂无知，匆匆少年。</p>
<h2 id="入门资源">入门资源</h2>
<p>我一直认为少而精才是正道，因此，我在这里只推荐三个资源，你看完，完全足够入门了：</p>
<ul>
<li>机器学习： 六维上有七月在线的课程，讲的很清晰， 过一遍，有个概念就好</li>
<li>深度学习： 吴恩达老师的课程还是很推荐的：<a href="https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm">吴恩达-深度学习</a></li>
<li>深度学习进阶： 李宏毅老师的课程十分推荐，够深， 不过需要翻墙：<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ">李宏毅课程</a>， 哔哩哔哩上应该也有人上传，可以找一找。</li>
<li>Pytorch：</li>
</ul>
<p><strong>pytorch-tutorial</strong></p>
<p><strong>pytorch-beginner</strong></p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch">code-of-learn-deep-learning-with-pytorch</a></p>
<p><strong>TextClassification-Pytorch</strong></p>
<p>这三个课程大概一个多月就足够了，看完之后， 你对深度学习有一个大致的了解，这个时候，你就可以看 Paper 了。</p>
<h2 id="paper-入门精选">Paper 入门精选</h2>
<p>这里的文章只选择了一些通用领域的，对于一些专用领域如对话， 多任务学习， 建议看对应的综述性文章。</p>
<p><strong>入门必看：</strong></p>
<ul>
<li>2018自然处理研究研究报告：这篇报告囊括了几乎全部的NLP任务，缺点是，不够前沿，涉及到的深度学习的内容并不多，但十分值得一看。</li>
</ul>
<pre><code>链接：https://pan.baidu.com/s/1Rwp84EddnaVxKULxs_RzSg 提取码：faws
</code></pre><ul>
<li>刘知远老师维护的仓库：主要讲述入门NLP领域时的诸多事项，主要偏向学术，强烈推荐。</li>
</ul>
<pre><code>https://github.com/zibuyu/research_tao
</code></pre><p><strong>大神综述</strong></p>
<ul>
<li>Deep learning （这是一篇论文，不是花书 )</li>
</ul>
<p><strong>基本神经单元</strong></p>
<p>首先，推荐张俊林大佬的一篇博客，这篇文章对比了 CNN, RNN, Transformer：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/54743941">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></li>
</ul>
<p>对于这些基本单元，有一篇中文Paper 讲的很详细，虽然没啥创新点：《深度学习研究综述》，推荐一看</p>
<p>一个入门的资料：<a href="https://link.zhihu.com/?target=https%3A//www.zybuluo.com/hanbingtao/note/48548">零基础入门深度学习</a>， 不过这些只要你好好看了吴恩达的课程和李宏毅老师的课程，这些基本不是问题。</p>
<p>关于 RNN， 主要是 LSTM 与 GRU 原理及二者之间的比较，看博客足够了：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/44106750">RNN ： NLP中最常见的神经网络单元</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/44163528">RNN 的梯度消失问题</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/44124492">LSTM：RNN最常用的变体</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55386469">RNN vs LSTM vs GRU &ndash; 该选哪个？</a></li>
</ul>
<p>关于CNN， 我个人没有看过早期的Paper，诸如 VGGNet， GoogleNet 等，如果感兴趣，你可以全都看看，我只看了最近的几篇文章如ResNet，DenseNet等。</p>
<ul>
<li>LeNet：Gradient-Based Learning Applied to Document Recognition （不推荐看）</li>
<li>AlexNet： ImageNet Classification with Deep Convolutional Neural Networks（不推荐看）</li>
<li>VGGNet：Very Deep Convolutional NetWorks for Large-Scale Image Recognition (不推荐看)</li>
<li>GoogleNet：Going Deeper with Convolutions （可不看）</li>
<li>ResNet：Deep Residual Learning for Image Recognition （必看）</li>
<li>DenseNet：Densely Connected Convolutional Networks （必看）</li>
</ul>
<p>上述之所以不推荐看的原因在于很多文章其实对于NLP领域用处不大，且 ResNet 的诞生基本上是打开了一个新天地，但有时间可以看看，万一有啥新灵感呢。</p>
<p>其余的一些 cnn 变体大多是用于图像领域，这里就不深入了，我也不会，，，</p>
<p><strong>语言模型</strong></p>
<ul>
<li>A Neural Probabilistic Language Model （经典，必看）</li>
<li>Efficient estimation of word representations in vector space （可不看， 主要讲解 Word2Vec 中 CBOW， Skip-Gram 模型）</li>
<li>Distributed Representations of Words and Phrases and their Compositionality （可不看，主要介绍Word2Vec中的几个优化Trick）</li>
<li>我早期的一篇文章： <a href="https://zhuanlan.zhihu.com/p/43453548">语言模型：从n元模型到NNLM</a> （其实早期的文章是写在自己的网站下的，后来，懒得充钱了，就迁移过来了，可以看出写作手法很稚嫩，毕竟是一年多以前的文章了）</li>
</ul>
<p>其实语言模型的经典文章有很多，但是，考虑到， 语言模型本身的研究价值在逐渐向预训练模型转移， 且如今情况下，预训练语言模型已经是土豪们玩的游戏，我等乞丐实验室还是早早避开，对大多数人来说，研究的价值有限。</p>
<p>如果你想研究预训练语言模型，那么语言模型的经典文章依旧有很强的研究价值，前提是，你有足够的资源，而绝大多数实验室和公司并没有。</p>
<p><strong>词向量</strong></p>
<p>推荐 Paper：</p>
<ul>
<li>Word2Vec： word2vec Parameter Learning Explained （一篇足够）</li>
<li>Glove： GloVe: Global Vectors for Word Representation （可以不看）</li>
<li>Fasttext：Bag of Tricks for Efficient Text Classification（可以不看）</li>
<li>Cove： Learned in Translation: Contextualized Word Vectors（可以不看，但推荐看）</li>
<li>ELMO：ELMO： Deep contextualized word representations（必看）</li>
</ul>
<p>之所以可以不看Glove 与 Fasttext， 是考虑到词向量的时代或许在接下来的几年中完全逝去， 没必要浪费太多时间在旧的方法上，用到再看也不迟。</p>
<p><strong>词向量与语言模型推荐两篇长文，写的很好，必看：</strong></p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//www.jiqizhixin.com/articles/2018-12-10-17">深度长文：NLP的巨人肩膀（上）</a></li>
</ul>
<p><strong>预训练语言模型</strong></p>
<p>首先推荐一篇文章：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//www.jiqizhixin.com/articles/2018-12-17-17">NLP 的巨人肩膀（下）：从 CoVe 到 BERT</a></li>
</ul>
<p>推荐Paper：</p>
<ul>
<li>ULMFit：Universal Language Model Fine-tuning for Text Classification（可不看，第一篇开山之作）</li>
<li>GPT-1： Improving Language Understanding by Generative Pre-Training（必看，改进，思路的完善）</li>
<li>Bert： BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding （必看，爆发）</li>
<li>GPT2：Language Models are Unsupervised Multitask Learners（必看，Bert 的改进）</li>
</ul>
<p>最近，对于 Bert 的一些改进文章：</p>
<ul>
<li>ERNIE： Enhanced Representation from kNowledge IntEgration（必看）</li>
<li>ERNIE：Enhanced Language Representation with Informative Entities （必看）</li>
<li>XLMs： Cross-lingual Language Model Pretraining（可不看）</li>
<li>MASS： Masked Sequence to Sequence Pre-training for language Generation （可不看）</li>
<li>UNILM： Unified Language Model Pre-training for Natural Language Understanding and Generation （可不看）</li>
</ul>
<p><strong>Sequence to Sequence</strong></p>
<p>其实就我个人看来， Seq2Seq 其实就是一个思想，并没有什么特殊之处，现在大家提到的也并不多了，其实都默认在许多模型之中了， 建议看看相关博客即可。</p>
<ul>
<li>Sequence to Sequence Learning with Neural Networks （可不看）</li>
</ul>
<p><strong>Attention</strong></p>
<p>Attention 部分的介绍可以看我的相关文章，这方面我看的相对多一些，毕竟阅读理解中各种花式 Attention 简直亮瞎眼睛：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/37601161">深度学习中的注意力模型（2017版）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59698165">NLP中的 Attention 机制（不定期更新）</a></li>
</ul>
<p>入门Paper：</p>
<ul>
<li>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention（可不看，也不推荐看）</li>
<li>Effective Approaches to Attention-based Neural Machine Translation（必看）</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate（必看）</li>
<li>Neural Responding Machine for Short-Text Conversation （可不看）</li>
</ul>
<p>Transformer 单独列出来是为了强调其重要性，下面三篇文章建议精读 ：</p>
<ul>
<li>Attention is all you need （必看）</li>
<li>Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures （可不看）</li>
<li><a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> (必看)</li>
<li><a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (必看)</li>
</ul>
<p>Attention 的文章很多，但大多是用在具体任务中的各种骚操作，不同的任务中采用的Attention是有较大区别的，具体的可以自己看相关领域的文章，如果你想了解超复杂的Attention， 欢迎关注阅读理解领域，保证让你欲生欲死，哈哈哈。</p>
<p><strong>Memory Networks</strong></p>
<p>这块目前的进展很慢， 还无法判断有没有研究价值，可以看看找找思路，当初也是比较火的方向，可是与Attention 并驾齐驱的，但现在看来，Attention 大行其道的今天，Memory Networks 已不复往日：</p>
<ul>
<li>memory networks （可不看）</li>
<li>End-to-end memory networks （可不看）</li>
</ul>
<p>关于 Attention 与 Memory Network 之间的关系可以参见：<a href="https://link.zhihu.com/?target=http%3A//www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Attention and Memory in Deep Learning and NLP</a></p>
<h2 id="调参相关">调参相关</h2>
<p>调参是一个经验和很耗时间的工作，因此这些论文大多都是给出建议，具体的还是要看你实际应用。</p>
<p><strong>优化算法</strong></p>
<ul>
<li>An overview of gradient descent optimization （必看）</li>
</ul>
<p>一些最近的优化算法，目前还没有人大规模使用，需要经过时间的检验，目前用的最多的依旧是文章中提到的几种算法，足够你掌握了。</p>
<p><strong>Dropout</strong></p>
<ul>
<li>Dropout ：A Simple Way to Prevent Neural Networks from Overfitting （必看）</li>
</ul>
<p><strong>激活函数</strong></p>
<ul>
<li>Comparing Deep Learning Activation Functions Across NLP tasks （必看）</li>
<li>Comparison of non-linear activation functions for deep neural networks on MNIST classification task （必看）</li>
</ul>
<p><strong>权重初始化</strong></p>
<ul>
<li>Xiver 初始化： Understanding the Difficult of Training Deep Feedforward Neural Networks （必看）</li>
<li>He 初始化：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication （必看）</li>
</ul>
<p><strong>层连接</strong></p>
<ul>
<li>ResNet ： Deep Residual Learning for Image Recognition （必看）</li>
<li>DenseNet： Densely Connected Convolutional Networks （必看）</li>
<li>Highway Networks （必看）</li>
</ul>
<p>这三篇文章涉及到的思想都很棒，有相关之处，都值得一看。</p>
<p><strong>Normalization</strong></p>
<ul>
<li>Batch Normalization：Batch normalization ： Accelerating deep network training by reducing internal covariate shift （必看）</li>
<li>How Does Batch Normalization Help Optimization （必看）</li>
</ul>
<p>其余的一些 Normalization， 可以等到研究的时候看，目前 NLP 中用到的并不多，主要是目前网络并不深。</p>
<p><strong>Tricks</strong></p>
<ul>
<li>Neural Networks: Tricks of the Trade ： 这本书比较老了，2012年的，但很多 Trick 目前还在使用，可以看看，知道都有啥玩意就行，有的Trick 毕竟已经被淘汰了。</li>
</ul>
<h2 id="最后">最后</h2>
<p>上述的所有 Paper 都更偏向于基础研究，对于上层应用如文本分类，阅读理解，机器翻译等因为涉及到的文章实在太多， 这里就不列举了，感兴趣的话，后期可以分别出专题来讨论。</p>
<p>其实还有很多文章没有推荐，一方面是考虑到这是倾向于对于入门NLPer的文章，不应该大规模的文章轰炸，另一方面，其余的一些文章更多的是基于上述文章的一些改进或者是一些Trick，重要性相对低一些。可以等大家入门之后，在自行钻研讨论。</p>
<p>写这么多，真的好累，好废时间，大家觉得写的凑合，就点个赞再走吧。</p>
</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script async src="//yihui.name/js/center-img.js"></script>
  
  <hr/>
  © <a href="https://imlauzh.github.io">Joseph Lau</a> 2017 &ndash; 2019 | <a href="https://github.com/imlauzh">Github</a>
  
  </footer>
  </body>
</html>

